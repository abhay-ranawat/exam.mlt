{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLT.Workbook.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "TOU1rVDuooc5",
        "84Z4RhwsPUjr",
        "_GG-q2Gq7ISZ",
        "jbY3bUmM7PnP",
        "PPkPzOvt7UtD",
        "uROKA5gHoxYZ",
        "QACAgZKsM4P9",
        "hJziX4IaM9kX",
        "FMkRrAg5OE2m",
        "VpCfbPWTOOWm",
        "F1UR_2hlO9rw",
        "mL2M5tARf4fc",
        "Q5iLaIW-gjdl",
        "xGpYO0bdggMs",
        "REluGAmurTmS",
        "P-UiNxMRV8jj",
        "_dkKongGYKlp",
        "4XwBw_QyYqHB",
        "lvtp7397YSL8",
        "kTdxF0gnZgda",
        "SWM-V80nYSf2",
        "d4RRKNeSaGpt",
        "0SNuzOc4YSrE",
        "EO856xBybL32",
        "sKz3NtCjZFhT",
        "qU24vQSQcX4n",
        "I7LDs4106mRJ",
        "zuIzDY2D6mRJ",
        "Xj9gPVHp6m1q",
        "oW1VS6ya6m1q",
        "28ZNrJEe6nUK",
        "uOnwiXoQAvfp",
        "pJZl2cvP6DlE",
        "yWxs5R-L6DlE",
        "BO4gIMj16DlF",
        "yRD092FAdnOO",
        "MDSzh-FyiPCk",
        "l9oc5frajs_3",
        "dylWTuMjklgS",
        "ayS-uACQi-gG",
        "j201DAPti-gG",
        "DiWoBpiUG9KO",
        "Yj4if5vdG9KP",
        "XeZP-3n8G-63",
        "Xf-YOzu6G-64",
        "oBM0iEIJG_2m",
        "Nm3elOy1ifIG",
        "u_H7c8lbifIH",
        "blVSMz17igGU",
        "8AkWfHFnigGV",
        "42dX8ncnzfo6",
        "hcyv-KNGzfo7",
        "Ov_xrC9kza_h",
        "h_ujBqlaza_i",
        "X1uwiPENzbM5",
        "3kjIlxWrzbM6",
        "_1p3wminzbaJ",
        "1FzBiS2KzbaK",
        "jkHWzX8V2uaR",
        "bbn2_Qjk2y_4",
        "0CnrVKXmihUI",
        "JpVp30_smdHA",
        "VySUeU2AzbpZ",
        "zagpNb2Szbpa",
        "zJhM67OxZKnE",
        "nDPS8nu6ZRAd",
        "tWPScmR4dkJj",
        "YDFLF_xMd2xZ",
        "FKrBzVBCeBFo",
        "7HP3oBS9h_Jp",
        "7BotIow0pD5F",
        "8TxCTpgbpJq3",
        "TkBgSLmtpOi4",
        "mkNA1pSdpOxJ",
        "MWEVwmQRpO-Z",
        "aqp-Co30bqKr",
        "7vSda6zLjhbc",
        "jBouLcz2j7Az",
        "L9DrRoLbkq-G",
        "iYv_Jpt0kYqG",
        "z7Wayk8PkY4m",
        "F89ibX8TmU1w",
        "uDjFK3H-pFy6",
        "_KM4C3ZVpFy7",
        "JE8L3yiXqFsR",
        "7SEdgWXxryV2",
        "pIKqFKfTs02A",
        "FkuBIibBtPWc",
        "PAu2oq6iryV4",
        "wVKkUBPxupvC",
        "LN2rvPM2vIHx",
        "nTGb_e8HupvD",
        "mj2NRnNhwSgD",
        "OeUheF0LwSgE",
        "sgYzRb_yxJPM",
        "ItQXfOtYxvkR",
        "vhFohiXzyHAm",
        "WoDLyNYjPnlf",
        "Kt5X1oOPP3q7",
        "7Rv1Gs6PP3q7",
        "qsuC6SFvP3q8",
        "pGQUw9NmP3q8",
        "-zd0yCukP3q9",
        "GwYtYPQoP3q-",
        "aAK7Er27P3q_",
        "9Veram85P3q_",
        "ZKV-NZBUP3q_",
        "47rSGIYsP3rA",
        "c-0OvA3ldFua",
        "CrFFTgzvd2RE",
        "rxKDEzKTd2RF",
        "b1lFOPOzd2RF",
        "ib4Mnu3uhxqo",
        "oXRg74abhxqp",
        "YMahunhGi4oh",
        "DaVPh8DIjG6X",
        "aTJzckuJjG6X",
        "vCB_1k31mv8f",
        "RrFNE-eJmv8g",
        "SVevK2ZRmv8h",
        "JrTayrKonxt5",
        "VEFDaf-Gnxt6",
        "dYqAWSh2nxt7",
        "YHVUdfTkpwlj",
        "c19cpLdBpwlk",
        "dPlgSKMJpwll",
        "iCNOJlcSsley",
        "LrxUOPVbslez",
        "LXuUxuTjslez",
        "VH4qE-XQuG_-",
        "sbmK1uxNuG__",
        "M-UjjXjfuG__",
        "MyWy1onjBhKt",
        "AjrsyYaEBrRF",
        "sZyHWeduBrRF",
        "_x__bWH8BrRF",
        "CpiUiiqSCgPf",
        "TE3oW_FnCgPg",
        "xPwzYxhyCjFn",
        "MgtUeXGrCjFn",
        "ZSp0BH4GDpqr",
        "dhztZt2TDpqs",
        "hLT_OnNoD7to",
        "Xpe7NkyvD7tp",
        "OxD3kl0YCXjp",
        "M5j3qWPBCXjq",
        "4M1jW6pYCXjq",
        "-25q0E71RKPV",
        "feuq6yViSbEO",
        "EE4Q1FDCSbEP",
        "YJUOxkk9Umzf",
        "-9oyqw4EUmzf",
        "kkqhyXwlHf37",
        "9PvW5g5iHf38",
        "wqxDue1HHf38",
        "vxRQb9PbbxLT",
        "xbRebicybe0N",
        "1Cl9AXlsbe0O",
        "LCAMIYQ8k1NU",
        "0i3SMAfPdMhV",
        "LYB6gLCAmgIg",
        "nBkmSLWRmwH2",
        "Rwt0SeaumwH3",
        "cOyos_twm-yC",
        "jHeCjxZGm-yD",
        "yJmyUAG0dMhW",
        "3jF7Ukbxn3Tq",
        "3ZbS9x9cn3Tr",
        "RN-EhHQWn3Ts",
        "HgZgxSb9n3Ts",
        "CrI-EIPbn3Tu",
        "Wb9Fl7zOn3Tw",
        "hvT5YeaPn3Tw",
        "EGFwdKPen3Tx",
        "ifteYR2Fk6KB",
        "8AHHKi00pXwP",
        "Tmt1XevYpXwQ",
        "-F267xPjpjA3",
        "kXXVuouck6ag",
        "U3ZqXOCXqNnz",
        "r1pF2QC6qNn0",
        "70-uPnEak6ah",
        "z6aHSNCqk6pX",
        "FJBMmbm1Icnd",
        "VJ2CSZinJrhI",
        "fo2DOI10JrhJ",
        "dDjCfV8rJrhJ",
        "EqByFP-CIgry",
        "-Kx93_HhIgry",
        "Sj784DmRIgry",
        "T8MgT2jaXnXE",
        "NPtatoSPXnXF",
        "RxOIKAE9Xn3U",
        "O8XBIQHnXn3V",
        "DaXxrF3fXoQD",
        "wPrB3t1sXoQE",
        "HKWFenJWxWUv",
        "HX5KLAWTxWUw",
        "TNFShKsvxWUx",
        "W1PU_VxwAeb2",
        "IvST53ypDWfX",
        "xNL8M2e4DWfX",
        "lFpqCBLnnAD_",
        "qkjbDR2jnAEA",
        "ffkkmuxVnAEA",
        "6saHR6Q8qZ7h",
        "ArDLegHrF_KC",
        "Al2ScLbwF_KD",
        "NG2cFP9yF_KD",
        "6-oc6TUoF_KE",
        "lTAsuZampjEx",
        "XD6UWKjmF5pt",
        "1z4bztGcCmLX",
        "vzPsDfni966h",
        "2rundbx17HX6",
        "9gBUBEoBrXk5",
        "SwR7qID_lSvQ",
        "AJfAorQcefB5",
        "wrYPwIEmej4X",
        "9KLtKhfA5GP-",
        "-m2ltGYZemJo"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###Week5: (Logistic Regression)"
      ],
      "metadata": {
        "id": "P-UiNxMRV8jj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_combination(X, w):\n",
        "    return X @ w\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def activation(X, w):\n",
        "    return sigmoid(linear_combination(X, w))\n",
        "\n",
        "def predict(X, w, threshold):\n",
        "    return np.where(activation(X, w) > threshold, 1, 0)\n",
        "\n",
        "def loss(y, sigmoid_vector, weight_vector, l1_reg_rate, l2_reg_rate):\n",
        "    loss_orig = -1 * np.sum(y * np.log(sigmoid_vector) + (1 - y) * np.log(1 - sigmoid_vector))\n",
        "    l2_reg = l2_reg_rate * np.dot(weight_vector.T, weight_vector)\n",
        "    l1_reg = l1_reg_rate * np.sum(np.abs(weight_vector))\n",
        "    return loss_orig + l1_reg + l2_reg\n",
        "\n",
        "def calculate_gradient(X, y, w, reg_rate):\n",
        "    return X.T @ (sigmoid(linear_combination(X, w)) - y) + reg_rate * w"
      ],
      "metadata": {
        "id": "nJUhkFajZIkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegression():\n",
        "    def set_weight_vector(self, w):\n",
        "        self.w = w\n",
        "    def linear_combination(self, X):\n",
        "        return X @ self.w\n",
        "    def sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "    def activation(self, X):\n",
        "        return self.sigmoid(self.linear_combination(X))\n",
        "    def predict(self, X, threshold=0.5):\n",
        "        return np.where(self.activation(X) > threshold, 1, 0)\n",
        "    def loss(self, X, y, reg_rate):\n",
        "        sigmoid_vector = self.activation(X)\n",
        "        loss_orig = -1 * np.sum(y * np.log(sigmoid_vector) + (1 - y) * np.log(1 - sigmoid_vector))\n",
        "        reg = reg_rate * np.dot(self.w.T, self.w)\n",
        "        return loss_orig + reg\n",
        "    def calculate_gradient(self, X, y, reg_rate):\n",
        "        return X.T @ (self.activation(X) - y) + reg_rate * self.w\n",
        "    def update_weights(self, grad, lr):\n",
        "        return self.w - grad * lr\n",
        "    def gd(self, X, y, num_epochs, lr, reg_rate):\n",
        "        self.w = np.zeros(X.shape[1])\n",
        "        self.w_all = []\n",
        "        self.err_all = []\n",
        "        for i in range(num_epochs):\n",
        "            grad = self.calculate_gradient(X, y, reg_rate)\n",
        "            self.w_all.append(self.w)\n",
        "            self.err_all.append(self.loss(X, y, reg_rate))\n",
        "            self.w = self.update_weights(grad, lr)\n",
        "        return self.w"
      ],
      "metadata": {
        "id": "DdxYuMFsZjH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools, functools\n",
        "def combinations(x, degree):\n",
        "    return itertools.combinations_with_replacement(x, degree)\n",
        "\n",
        "def compute_new_features(items):\n",
        "    return functools.reduce(lambda x, y: x * y, items)\n",
        "\n",
        "def polynomial_transform(x, degree):\n",
        "    if x.ndim == 1:\n",
        "        x = x[:, None]\n",
        "\n",
        "    features = [np.ones(len(x))] #a list of np-arrays with all 1.\n",
        "\n",
        "    for degree in range(1, degree + 1):\n",
        "        for item in combinations(x.T, degree):\n",
        "            features.append(compute_new_features(item))\n",
        "            \n",
        "    return np.asarray(features).T\n",
        "def plot_learning_curve(err):\n",
        "    plt.plot(np.arange(len(err)), err, 'r-')"
      ],
      "metadata": {
        "id": "ao1bnTBuaIia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_toy_data():\n",
        "    x0 = np.random.normal(size=50).reshape(-1, 2) - 1\n",
        "    x1 = np.random.normal(size=50).reshape(-1, 2) + 1\n",
        "    return np.concatenate([x0, x1]), np.concatenate([np.zeros(25), np.ones(25)]).astype(np.int)\n",
        "\n",
        "sns.set_context(context='notebook',font_scale=1.5,rc={\"lines.linewidth\":2.5})\n",
        "\n",
        "def visualize_model(X_train, labels, lsc, degree=1):\n",
        "    f = plt.figure(figsize=(8,8))\n",
        "\n",
        "    x1_min = np.min(x_train[:,1])\n",
        "    x1_max = np.max(x_train[:,1])\n",
        "    x2_min = np.min(x_train[:,2])\n",
        "    x2_max = np.max(x_train[:,2])\n",
        "    \n",
        "    x1_test, x2_test = np.meshgrid(np.linspace(x1_min, x1_max, 100), np.linspace(x2_min, x2_max, 100)) \n",
        "    x_test = np.array([x1_test, x2_test]).reshape(2,-1).T\n",
        "    x_test_poly = polynomial_transform(x_test, degree=degree)\n",
        "    y_test = lsc.predict(x_test_poly, 0.5)\n",
        "    \n",
        "    sns.scatterplot(data=x_train, x=x_train[:,1], y=x_train[:,2], hue=labels)\n",
        "    plt.contourf(x1_test, x2_test, y_test.reshape(100,100), alpha=0.5, levels=np.linspace(0,1,3))\n",
        "    plt.gca().set_aspect('equal', adjustable='box')"
      ],
      "metadata": {
        "id": "oVFvLRg3bNoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Week6:(Naive Bayes)"
      ],
      "metadata": {
        "id": "sKz3NtCjZFhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(X, y):\n",
        "    alpha = 1 #Laplace correction\n",
        "    n_samples, n_features = X.shape\n",
        "    n_classes = len(np.unique(y))\n",
        "    w = np.zeros((n_classes, n_features), dtype=np.float64)\n",
        "    w_priors = np.zeros(n_classes, dtype=np.float64)\n",
        "    \n",
        "    for c in range(n_classes):\n",
        "        X_c = X[y == c]\n",
        "        w[c, :] = (np.sum(X_c, axis=0) + alpha) / (X_c.shape[0] + n_classes * alpha)\n",
        "        w_priors[c] = (X_c.shape[0] + alpha) / (float(n_samples) + n_classes * alpha)\n",
        "        \n",
        "    print(\"Weight vector:\", w)\n",
        "    print(\"Prior\", w_priors)\n",
        "    return w, w_priors"
      ],
      "metadata": {
        "id": "BrI7mVIaaRsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BernoulliNB():\n",
        "    def __init__(self, alpha=1.0):\n",
        "        self.alpha = alpha\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        n_classes = len(np.unique(y))\n",
        "        self.w = np.zeros((n_classes, n_features), dtype=np.float64)\n",
        "        self.w_priors = np.zeros(n_classes, dtype=np.float64)\n",
        "\n",
        "        for c in range(n_classes):\n",
        "            X_c = X[y == c]\n",
        "            self.w[c, :] = (np.sum(X_c, axis=0) + self.alpha) / (X_c.shape[0] + n_classes * self.alpha)\n",
        "            self.w_priors[c] = (X_c.shape[0] + self.alpha) / (float(n_samples) + n_classes * self.alpha)\n",
        "\n",
        "        print(\"Class Conditional Density:\", self.w)\n",
        "        print(\"Prior\", self.w_priors)\n",
        "\n",
        "    def log_likelihood_prior_prod(self, X):\n",
        "        return X @ (np.log(self.w).T) + (1 - X) @ np.log((1 - self.w).T) + np.log(self.w_priors)\n",
        "    \n",
        "    def predict_proba(self, X):\n",
        "        q = self.log_likelihood_prior_prod(X)\n",
        "        return np.exp(q) / np.expand_dims(np.sum(np.exp(q), axis=1), axis=1)\n",
        "    \n",
        "    def predict(self, X):\n",
        "        q = self.log_likelihood_prior_prod(X)\n",
        "        return np.argmax(q, axis=1)"
      ],
      "metadata": {
        "id": "_pQ_7kkHa-TM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GaussianNB():\n",
        "    def __init__(self, alpha=1.0):\n",
        "        self.alpha = alpha\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self._classes = np.unique(y)\n",
        "        n_classes = len(self._classes)\n",
        "        self._mean = np.zeros((n_classes, n_features), dtype=np.float64)\n",
        "        self._var = np.zeros((n_classes, n_features), dtype=np.float64)\n",
        "        self._priors = np.zeros(n_classes, dtype=np.float64)\n",
        "\n",
        "        for idx, c in enumerate(self._classes):\n",
        "            X_c = X[y == c]\n",
        "            self._mean[idx, :] = X_c.mean(axis=0)\n",
        "            self._var[c] = X_c.var(axis=0)\n",
        "            self._priors[idx] = X_c.shape[0] / float(n_samples)\n",
        "\n",
        "        print(\"Mean:\", self._mean)\n",
        "        print(\"Variance:\", self._var)\n",
        "        print(\"Prior\", self._priors)\n",
        "    \n",
        "    def _calc_pdf(self, class_idx, X):\n",
        "        mean = self._mean[class_idx]\n",
        "        var = np.diag(self._var[class_idx])\n",
        "        z = np.power(2 * np.pi, X.shape[0] / 2) * np.power(np.linalg.det(var), 1/2)\n",
        "        return (1/z) * np.exp(-0.5 * (X - mean).T @ (np.linalg.inv(var)) @ (X - mean))\n",
        "    \n",
        "    def _calc_prod_likelihood_prior(self, X):\n",
        "        self.q = np.zeros((X.shape[0], len(self._classes)), dtype=np.float64)\n",
        "        for x_idx, x in enumerate(X):\n",
        "            for idx, c in enumerate(self._classes):\n",
        "                self.q[x_idx, c] = (np.log(self._calc_pdf(idx, x)) \n",
        "                                                         + np.log(self._priors[idx]))\n",
        "    \n",
        "    def predict_proba(self, X):\n",
        "        self._calc_prod_likelihood_prior(X)\n",
        "        return np.exp(self.q) / np.expand_dims(np.sum(np.exp(self.q), axis=1), axis=1)\n",
        "    \n",
        "    def predict(self, X):\n",
        "        self._calc_prod_likelihood_prior(X)\n",
        "        return np.argmax(self.q, axis=1)"
      ],
      "metadata": {
        "id": "HyA0Ck8ybtR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultinomialNB():\n",
        "    def __init__(self, alpha=1.0):\n",
        "        self.alpha = alpha\n",
        "        \n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self._classes = np.unique(y)\n",
        "        n_classes = len(self._classes)\n",
        "        self.w = np.zeros((n_classes, n_features), dtype=np.float64)\n",
        "        self.w_priors = np.zeros(n_classes, dtype=np.float64)\n",
        "\n",
        "        for idx, c in enumerate(self._classes):\n",
        "            X_c = X[y == c]\n",
        "            total_count = np.sum(np.sum(X_c, axis=1))\n",
        "            self.w[idx, :] = (np.sum(X_c, axis=0) + self.alpha) / (total_count + n_classes * self.alpha)\n",
        "            self.w_priors[idx] = (X_c.shape[0] + self.alpha) / (float(n_samples) + n_classes * self.alpha)\n",
        "\n",
        "    def log_likelihood_prior_prod(self, X):\n",
        "        return X @ (np.log(self.w).T) + np.log(self.w_priors)\n",
        "    \n",
        "    def predict_proba(self, X):\n",
        "        q = self.log_likelihood_prior_prod(X)\n",
        "        return np.exp(q) / np.expand_dims(np.sum(np.exp(q), axis=1), axis=1)\n",
        "    \n",
        "    def predict(self, X):\n",
        "        q = self.log_likelihood_prior_prod(X)\n",
        "        return np.argmax(q, axis=1)"
      ],
      "metadata": {
        "id": "4wJ9Ok9ZcR8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Week7:(Softmax Regression, KNN)"
      ],
      "metadata": {
        "id": "jrXoS3Pd6ei8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For softmax regression, we've to one-hot encode y\n",
        "def convert_to_one_hot_encoding(y, k):\n",
        "    #k = len(np.unique(y)) #get value of k (classes) \n",
        "    y_unique = np.unique(y) # array of unique classes\n",
        "    y_sort = sorted(y_unique) #sort the classes\n",
        "    for i in range(len(y_unique)):\n",
        "        y[i] = y_sort.index([y[i]])\n",
        "    print(y) #converts to values starting from 0\n",
        "    y_one_hot = np.zeros((len(y),k)) #initialize\n",
        "    y_one_hot[np.arange(len(y)), y] = 1 #one hot rncode\n",
        "    \n",
        "    return y_one_hot"
      ],
      "metadata": {
        "id": "WolX5vtO6ei8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_combination(X, w, b):\n",
        "    return X @ w + b\n",
        "def softmax(z):\n",
        "    # subtracting max of z for numerical stability\n",
        "    exp = np.exp(z - np.max(z))\n",
        "    for i in range(len(z)):\n",
        "        exp[i] /= np.sum(exp[i])\n",
        "    \n",
        "    return exp\n"
      ],
      "metadata": {
        "id": "Bsh8GVQy6mRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(X, y, lr, k, epochs):\n",
        "    n, m = X.shape\n",
        "    \n",
        "    w = np.random.random((m, k))\n",
        "    b = np.random.random(k)\n",
        "    \n",
        "    losses = []\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        z = linear_combination(X, w, b)\n",
        "        y_hat = softmax(z)\n",
        "        y_hot = convert_to_one_hot_encoding(y, k)\n",
        "        \n",
        "        w_grad = (1/n) * (X.T @ (y_hat - y_hot))\n",
        "        b_grad = (1/n) * np.sum(y_hat - y_hot)\n",
        "        \n",
        "        w = w - lr * w_grad\n",
        "        b = b - lr * b_grad\n",
        "        \n",
        "        loss = -np.mean(np.log(y_hat[np.arange(len(y)), y]))\n",
        "        losses.append(loss)\n",
        "    return w, b, losses\n",
        "\n",
        "def predict(X, w, b):\n",
        "    z = X @ w + b\n",
        "    y_hat = softmax(z)\n",
        "    return np.argmax(y_hat, axis=1)\n",
        "\n",
        "def accuracy(y, y_hat):\n",
        "    return np.sum(y == y_hat) / len(y)\n",
        "\n"
      ],
      "metadata": {
        "id": "L5O7MKOk6mRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def EuclideanDistance(x1,x2):\n",
        "  dist = np.sum((x1-x2)**2,axis=1)\n",
        "  return dist\n",
        "\n",
        "def ManhattanDistance(x1,x2): \n",
        "  np.sum(np.abs(x1-x2),axis=1)"
      ],
      "metadata": {
        "id": "5rK0hMdc6m1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class KNN:\n",
        "  def __init__(self,k,distance_metric=EuclideanDistance,task_type=\"Classification\"):\n",
        "    self._k = k \n",
        "    self._distance_metric = distance_metric \n",
        "    self._task_type = task_type \n",
        "  \n",
        "  def fit(self,X,y):\n",
        "    #fitting model on data only requires copying the data\n",
        "    self._X = X \n",
        "    self._y = y \n",
        "  def predict(self,newExample):\n",
        "\n",
        "    #newExample is example for which prediction has to be made.\n",
        "    ''' label: predicted label for newExample\n",
        "    k_nearest_neighbours_indices : indices of the nearest k neighbours\n",
        "\n",
        "    '''\n",
        "    #2. calculate the distance between the new example and every example from \n",
        "    # data. Thus create a distance vector.\n",
        "\n",
        "    distance_vector = self._distance_metric(self._X,newExample)\n",
        "\n",
        "    # Get indices of nearest k neighbours\n",
        "\n",
        "    k_nearest_neighbours_indices = np.argpartition(distance_vector,self._k)[:self._k]\n",
        "    #4 Get the labels of the selected k entries.\n",
        "    k_nearest_neighbours = self._y[k_nearest_neighbours_indices]\n",
        "    # If it is a classification task, return the majority class by computing mode \n",
        "    #the k labels\n",
        "    if self._task_type == 'Classification':\n",
        "      label = stats.mode(k_nearest_neighbours)[0] \n",
        "    else:\n",
        "      label = k_nearest_neighbours.mean() \n",
        "    \n",
        "    return label, k_nearest_neighbours_indices \n",
        "\n",
        "  def eval(self, X_test,y_test):\n",
        "    if self._task_type == 'Classification':\n",
        "      y_predicted = np.zeros(y_test.shape) \n",
        "      for i in range(y_test.shape[0]):\n",
        "        y_predicted[i],_ = self.predict(X_test[i,:])\n",
        "      error = np.mean(y_test==y_predicted, axis = 0)\n",
        "    \n",
        "    else:\n",
        "      y_predicted = np.zeros(y_test.shape) \n",
        "      for i in range(y_test.shape[0]):\n",
        "        y_predicted[i],_ = self.predict(X_test[i,:]) \n",
        "        error_vector = y_predicted - y_test \n",
        "        error = np.sqrt((error_vector.T@error_vector)/error_vector.ravel().shape[0])\n",
        "    return error\n",
        "    \n",
        "\n"
      ],
      "metadata": {
        "id": "fRXqVOEw6nUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def draw_decision_boundary(model, axis_chart, num_points=201, opacity=0.05):\n",
        "  tx = np.linspace(np.min(model._X[:,0],axis=0)-2,\n",
        "                   np.max(model._X[:,0],axis=0)+2, num_points)\n",
        "  ty = np.linspace(np.min(model._X[:,0],axis=0)-2,\n",
        "                   np.max(model._X[:,0],axis=0)+2, num_points) \n",
        "  xx,yy = np.meshgrid(tx,ty) \n",
        "  grid_prediction = np.zeros(xx.shape) \n",
        "\n",
        "  for i in range(num_points):\n",
        "    for j in range(num_points):\n",
        "      grid_prediction[i][j],_ = model.predict([xx[i][j],yy[i][j]]) \n",
        "  \n",
        "  axis_chart.scatter(xx.ravel(),yy.ravel(),c=grid_prediction.ravel(),alpha=opacity)\n",
        "  "
      ],
      "metadata": {
        "id": "viKTETP6Avfq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def maketwospirals(num_points=1000,rotations =2, noise = .5):\n",
        "  ''' \n",
        "  Parameters:\n",
        "  num_points: Number of points to be generated per spiral\n",
        "  rotations : How many times a spiral should rotate\n",
        "  noise: noise factor \n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  coordinates and class vectors of the two spirals \n",
        "\n",
        "\n",
        "  ''' \n",
        "  n = np.sqrt(np.random.rand(num_points,1))*rotations * (2*np.pi)\n",
        "  rx = -np.cos(n)*n + np.random.rand(num_points,1) * noise \n",
        "  ry =  np.sin(n)*n + np.random.rand(num_points,1)*noise\n",
        "\n",
        "  return (np.concatenate((np.hstack((rx,ry)),\n",
        "                          np.hstack((-rx,-ry))),\n",
        "                         axis=0),\n",
        "          np.concatenate((np.zeros(num_points),\n",
        "                          np.ones(num_points)),\n",
        "                         axis=0)) \n",
        "  "
      ],
      "metadata": {
        "id": "jLvD-VeqA-SL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Week8:(SVM)"
      ],
      "metadata": {
        "id": "pJZl2cvP6DlE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class softSVM:\n",
        "\n",
        "  def __init__(self,C):\n",
        "    self._support_vectors = None \n",
        "    self.C = C\n",
        "    self.w = None \n",
        "    self.b = None \n",
        "    self.X = None \n",
        "    self.y = None \n",
        "\n",
        "    # n is the number of data points \n",
        "    self.n = 0 \n",
        "    # d is the number of dimensions \n",
        "    self.d = 0 \n",
        "\n",
        "  def __decision_function(self,X):\n",
        "    return X.dot(self.w) + self.b \n",
        "\n",
        "  def __cost(self,margin):\n",
        "    return (1/2)*(self.w).dot(self.w) + self.C*np.sum(np.maximum(0,1-margin))\n",
        "  \n",
        "  def __margin(self, X,y):\n",
        "    return y*self.__decision_function(X) \n",
        "\n",
        "  def fit(self,X,y, lr=1e-3, epochs=500):\n",
        "    # Initialize w and b \n",
        "    self.n, self.d = X.shape \n",
        "    self.w = np.random.randn(self.d) \n",
        "    self.b = 0 \n",
        "\n",
        "    #required only for plotting \n",
        "    self.X = X \n",
        "    self.y = y \n",
        "\n",
        "    loss_array = []\n",
        "    \n",
        "    for _ in range(epochs):\n",
        "      margin = self.__margin(X,y) \n",
        "      loss = self.__cost(margin) \n",
        "      loss_array.append(loss)\n",
        "\n",
        "      missclassified_pts_idx = np.where(margin < 1)[0] \n",
        "      d_w = self.w - self.C * y[missclassified_pts_idx].dot(X[missclassified_pts_idx]) \n",
        "\n",
        "      self.w = self.w - lr * d_w \n",
        "\n",
        "      d_b = -self.C * np.sum(y[missclassified_pts_idx]) \n",
        "      self.b = self.b - lr * d_b \n",
        "    self._support_vectors = np.where(self.__margin(X,y) <= 1)[0] \n",
        "\n",
        "  def predict(self,X):\n",
        "    return np.sign(self.__decision_function(X)) \n",
        "\n",
        "  def score(self,X,y):\n",
        "    p = self.predict(X) \n",
        "    return np.mean(y==p) \n",
        "\n",
        "  def plot_decision_boundary(self):\n",
        "    plt.scatter(self.X[:,0],self.X[:,1],c = self.y, marker='o',s = 100, cmap = 'autumn') \n",
        "\n",
        "    ax = plt.gca() \n",
        "    xlim = ax.get_xlim() \n",
        "    ylim = ax.get_ylim() \n",
        "\n",
        "    #create grid to evaluate model \n",
        "\n",
        "    xx = np.linspace(xlim[0],xlim[1],30) \n",
        "    yy = np.linspace(ylim[0],ylim[1],30) \n",
        "    YY,XX = np.meshgrid(yy,xx) \n",
        "    xy = np.vstack([XX.ravel(),YY.ravel()]).T \n",
        "    z = self.__decision_function(xy).reshape(XX.shape) \n",
        "\n",
        "    #plot decision boundary and margins \n",
        "\n",
        "    ax.contour(XX,YY,z, colors=['g','k','g'],levels = [-1,0,1],\n",
        "               linestyles=['--','-','--'],linewidths = [2.0,2.0,2.0]) \n",
        "\n",
        "    #highlight the support vectors \n",
        "\n",
        "    ax.scatter(self.X[:,0][self._support_vectors],\n",
        "               self.X[:,1][self._support_vectors],s =250,\n",
        "               linewidth =1 , facecolors = 'none',edgecolors = 'k') \n",
        "\n",
        "    plt.xlabel('x1') \n",
        "    plt.ylabel('x2') \n",
        "\n",
        "    plt.show() "
      ],
      "metadata": {
        "id": "TbRABjO9huKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X,Y = make_blobs(n_samples=60,n_features =2, centers = [[0,0],[6,6]],\n",
        "                 cluster_std=1,\n",
        "                 random_state=12)\n",
        "\n",
        "Y = np.where(Y == 0 , -1 ,1) \n",
        "\n",
        "#plot the two classes \n",
        "plt.figure(figsize=(8,8)) \n",
        "plt.scatter(X[:,0],X[:,1],marker = 'o', c =Y , s =200, edgecolor='k') \n",
        "plt.xlabel('x1',fontsize=20) \n",
        "plt.ylabel('x2',fontsize=20) \n",
        "plt.title('Data points',fontsize = 20) \n",
        "plt.show()"
      ],
      "metadata": {
        "id": "c67gsaGZiXqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "svm = softSVM(C=1) \n",
        "svm.fit(X,Y) \n",
        "\n",
        "print(svm.w) \n",
        "def get_hyperplane_value(x,w,b,offset):\n",
        "  return -1*(w[0]*x+b+offset)/w[1] \n",
        "\n",
        "fig = plt.figure(figsize=(8,8)) \n",
        "ax = fig.add_subplot(1,1,1) \n",
        "plt.scatter(X[:,0],X[:,1],marker='o',c=Y, s =200,edgecolor = 'k',cmap='autumn') \n",
        "x0_1 = np.amin(X[:,0]) \n",
        "x0_2 = np.amax(X[:,0]) \n",
        "\n",
        "x1_1 = get_hyperplane_value(x0_1, svm.w, svm.b,0)\n",
        "x1_2 = get_hyperplane_value(x0_2, svm.w, svm.b,0)\n",
        "\n",
        "x1_1_m = get_hyperplane_value(x0_1, svm.w, svm.b,-1)\n",
        "x1_2_m = get_hyperplane_value(x0_2, svm.w, svm.b,-1)\n",
        "\n",
        "x1_1_p = get_hyperplane_value(x0_1, svm.w, svm.b,1)\n",
        "x1_2_p = get_hyperplane_value(x0_2, svm.w, svm.b,1) \n",
        "\n",
        "\n",
        "ax.plot([x0_1,x0_2],[x1_1,x1_2],'y') \n",
        "ax.plot([x0_1,x0_2],[x1_1_m,x1_2_m],'k--') \n",
        "ax.plot([x0_1,x0_2],[x1_1_p,x1_2_p],'k--')\n",
        "\n",
        "x1_min = np.amin(X[:,1]) \n",
        "x1_max = np.amax(X[:,1]) \n",
        "ax.set_ylim([x1_min -3, x1_max + 3]) \n",
        "\n",
        "ax.legend(['Hyperplane','Bounding plane 1', 'Bounding plane 2'],loc='center left', bbox_to_anchor=(1,0.5))\n",
        "plt.xlabel('x1') \n",
        "plt.ylabel('x2') \n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Wqgryf2pjLGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SVMDualProblem:\n",
        "\n",
        "  def __init__(self,C=1.0, kernel='rbf', sigma=0.1,degree =2):\n",
        "    self.C = C \n",
        "    if kernel == 'poly':\n",
        "      self.kernel= self._polynomial_kernel\n",
        "      self.c = 1\n",
        "      self.degree = degree \n",
        "\n",
        "    else: \n",
        "      self.kernel = self._rbf_kernel \n",
        "      self.sigma = sigma \n",
        "\n",
        "    self.X = None \n",
        "    self.y = None \n",
        "    self.alpha = None \n",
        "    self.b = 0 \n",
        "    self.ones = None \n",
        "\n",
        "  def _rbf_kernel(self,X1,X2):\n",
        "    return np.exp(-1(1/self.sigma**2)*np.linalg.norm(X1[:,np.newaxis]-X2[np.newaxis,:],axis=2)**2) \n",
        "\n",
        "  def _polynomial_kernel(self, X1,X2):\n",
        "    return (self.c + X1.dot(X2.T))**self.degree \n",
        "  \n",
        "  def fit(self, X, y, lr =1e-3, epochs =100):\n",
        "\n",
        "    self.X = X\n",
        "    self.y = y \n",
        "\n",
        "    self.alpha = np.random.random(X.shape[0]) \n",
        "    self.b = 0\n",
        "\n",
        "    self.ones = np.ones(X.shape[0]) \n",
        "\n",
        "    y_iy_jk_ij = np.outer(y,y)*self.kernel(X,X) \n",
        "\n",
        "    losses = [] \n",
        "\n",
        "    for _ in range(epochs):\n",
        "      gradient = self.ones - y_iy_jk_ij.dot(self.alpha) \n",
        "\n",
        "      self.alpha = self.alpha + lr*gradient \n",
        "\n",
        "      self.alpha[self.alpha > self.C] = self.C \n",
        "      self.alpha[self.alpha < 0] = 0 \n",
        "\n",
        "      loss = np.sum(self.alpha) -0.5 * np.sum(\n",
        "          np.outer(self.alpha,self.alpha)*y_iy_jk_ij) \n",
        "      \n",
        "      losses.append(loss) \n",
        "\n",
        "    index = np.where((self.alpha)>0 & (self.alpha < self.C))[0] \n",
        "\n",
        "    b_i = y[index] - (self.alpha*y).dot(self.kernel(X,X[index])) \n",
        "\n",
        "    #alternative code \n",
        "    # b_1 = y[index] - np.sum((self.alpha*y).reshape(-1,1)*self.kernel(X,X[index]),axis =0)\n",
        "\n",
        "    self.b = np.mean(b_i) \n",
        "    plt.plot(losses) \n",
        "    plt.title(\"loss per epochs\") \n",
        "    plt.show() \n",
        "\n",
        "  def __decision_function(self,X):\n",
        "    return (self.alpha*self.y).dot(self.kernel(self.X,X)) + self.b \n",
        "\n",
        "  def predict(self,X):\n",
        "    return np.sign(self.__decision_function(X))\n",
        "\n",
        "  def score(self,X,y):\n",
        "    y_hat = self.predict(X) \n",
        "    return np.mean(y==y_hat) \n",
        "\n",
        "  def plot_decision_boundary(self):\n",
        "    plt.scatter(self.X[:,0],self.X[:,1], c = self.y, s=50, cmap = plt.cm.Paired, alpha =0.5) \n",
        "    ax = plt.gca() \n",
        "\n",
        "    xlim = ax.get_xlim()\n",
        "    ylim = ax.get_ylim() \n",
        "\n",
        "    #create grid to evaluate model \n",
        "    xx = np.linspace(xlim[0],xlim[1],30)\n",
        "    yy = np.linspace(ylim[0],ylim[1],30) \n",
        "    YY,XX = np.meshgrid(yy,xx) \n",
        "    xy = np.vstack([XX.ravel(),YY.ravel()]).T \n",
        "    z = self.__decision_function(xy).reshape(XX.shape) \n",
        "\n",
        "    #plot decision boundary and margins \n",
        "\n",
        "    ax.contour(XX,YY, z, colors=['b','g','r'],levels = [-1,0,1],alpha=0.5,\n",
        "               linestyles = ['--','-','--'],linewidths = [2.0,2.0,2.0]) \n",
        "    ax.scatter(self.X[:,0][self.alpha > 0.],self.X[:,1][self.alpha > 0.],s=50,\n",
        "               linewidths=1, facecolors ='none', edgecolors = 'k') \n",
        "    plt.show()\n",
        "\n",
        " \n",
        "    "
      ],
      "metadata": {
        "id": "WaLv234Yjs_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Week9"
      ],
      "metadata": {
        "id": "gbpJdLDw4EFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "nlIvlaVwHx6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_entropy_whole(df):\n",
        "    #last column in the dataframe is target variable\n",
        "    target = df.keys()[-1]\n",
        "\n",
        "    #initialization\n",
        "    overall_entropy = 0\n",
        "\n",
        "    #possible values of the target\n",
        "    values_in_target = df[target].unique()\n",
        "    \n",
        "    for value in values_in_target:\n",
        "        p = df[target].value_counts()[value] / len(df[target])\n",
        "        overall_entropy += -p * np.log2(p)\n",
        "    return overall_entropy"
      ],
      "metadata": {
        "id": "XSqh6gAi6DlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_entropy_of_attribute(df, attribute):\n",
        "    #last column in dataframe is label\n",
        "    target = df.keys()[-1]\n",
        "\n",
        "    #possible values of the target\n",
        "    values_in_target = df[target].unique()\n",
        "    \n",
        "    #this gives different features in that attribute (\n",
        "        # like hot, cold in temperature )  \n",
        "    values_in_attribute = df[attribute].unique()\n",
        "\n",
        "    #initialize attribute entropy\n",
        "    entropy_attribute = 0\n",
        "\n",
        "    #for loop implementation\n",
        "    for value_in_attribute in values_in_attribute:\n",
        "        overall_entropy = 0\n",
        "        for value_in_target in values_in_target:\n",
        "            num = len(df[attribute][df[attribute] == value_in_attribute][df[target] == value_in_target])\n",
        "            den = len(df[attribute][df[attribute] == value_in_attribute])\n",
        "            p = num / (den + eps)\n",
        "            overall_entropy += -p * np.log2(p + eps)\n",
        "        p2 = den / len(df)\n",
        "        entropy_attribute += -p2 * overall_entropy\n",
        "    return abs(entropy_attribute)"
      ],
      "metadata": {
        "id": "aUPpeXAXG9KQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for attribute in df.keys()[:-1]:\n",
        "    print(f'Entropy of the attribute \"{attribute}\" is :', find_entropy_of_attribute(df, attribute))"
      ],
      "metadata": {
        "id": "lExrLciCG9KQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_best_attribute_to_divide(df):\n",
        "    #information gain initialization\n",
        "    IG = []\n",
        "    #get all column names\n",
        "    all_attributes = df.keys()[:-1]\n",
        "\n",
        "    #get information gain for every attribute\n",
        "    for attribute in all_attributes:\n",
        "        IG.append(find_entropy_whole(df) - find_entropy_of_attribute(df, attribute))\n",
        "    \n",
        "    #get the index of attribute with best information gain\n",
        "    index_of_attribute_with_max_IG = np.argmax(IG)\n",
        "\n",
        "    #print index of attribute with maximum gain\n",
        "    best_attribute = all_attributes[index_of_attribute_with_max_IG]\n",
        "    return best_attribute\n"
      ],
      "metadata": {
        "id": "ryA9rbC6G-65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def buildTree(df, tree=None):\n",
        "    #last column in our dataframe\n",
        "    target = df.keys()[-1]\n",
        "\n",
        "    #get attribute with maximum information gain\n",
        "    node = find_best_attribute_to_divide(df)\n",
        "\n",
        "    #get distinct value of that attribute\n",
        "    attValue = np.unique(df[node])\n",
        "\n",
        "    #create an empty dictionary to create tree\n",
        "    if tree is None:\n",
        "        tree = {}\n",
        "        tree[node] = {}\n",
        "\n",
        "    #we make a loop to construct a tree by calling the function recursively \n",
        "    #we check if the subset is pure, we stop if pure\n",
        "    for value in attValue:\n",
        "        subtable = df[df[node] == value].reset_index(drop=True)\n",
        "        clValue, counts = np.unique(subtable['play'], return_counts=True)\n",
        "        if len(counts) == 1: #Checking purity of subset\n",
        "            tree[node][value] = clValue[0]\n",
        "        else:\n",
        "            tree[node][value] = buildTree(subtable) # Calling the function recusively\n",
        "    return tree\n",
        "\n"
      ],
      "metadata": {
        "id": "umJTTnf_G_2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Week10: (Random Forest, Gradient Boosting)\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Nm3elOy1ifIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def bag (X,y):\n",
        "  #Counts the number or rows in the feature matrix\n",
        "  n_samples = X.shape[0] \n",
        "\n",
        "  #Generates a random sample from the  given input.\n",
        "  indices = np.random.choice(n_samples,size = n_samples,replace=True,random_state=1)\n",
        "  # Note that the second argument size has been set to the size of \n",
        "  # the original sample dataset and replacement has been set to True\n",
        "\n",
        "  return X[indices], y[indices]"
      ],
      "metadata": {
        "id": "Ajv2d5SxigGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def most_common_label(y):\n",
        "  counter = Counter(y)\n",
        "  most_common = counter.most_common(1)[0][0] \n",
        "  return most_common\n",
        "\n",
        "\n",
        "y = [1,1,1,0,0,2,2,2,2,3,3,3]\n",
        "print(Counter(y))\n",
        "print(Counter(y).most_common(2)[0][0])\n"
      ],
      "metadata": {
        "id": "LxPIz_x4zfo7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomForest:\n",
        "  def __init__(self, n_trees=10, min_samples_split=2, max_depth=100, max_features=None):\n",
        "    self.n_trees = n_trees #hyperparameter for fixing number of trees to be generated\n",
        "    self.min_samples_split = min_samples_split # min no of samples required for split\n",
        "    self.max_depth=max_depth #maximum depth of decision tree\n",
        "    self.max_features = max_features #maximum no of features to be considered\n",
        "    self.trees = [] "
      ],
      "metadata": {
        "id": "uJGvK_Ncza_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(self,X,y):\n",
        "  self.trees =[] #Empty array of trees which gets filled in during operations.\n",
        "  for _ in range(self.n_trees): #we are using underscore we are just repeating the operations.\n",
        "    tree = DecisionTreeClassifier( #we will now make RF class inherit features from Sklearn'\n",
        "                                  min_samples_split = self.min_samples_split,\n",
        "                                  max_depth = self.max_depth,\n",
        "                                  max_features=self.max_features\n",
        "                                \n",
        "        \n",
        "    )\n",
        "    X_sample, y_sample = bag(X,y) \n",
        "    tree.fit(X_sample,y_sample) \n",
        "    self.trees.append(tree) # we will append each of these tree.\n",
        "    \n"
      ],
      "metadata": {
        "id": "Cd865jZY1JvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(self,X):\n",
        "  tree_predict = np.array([tree.predict(X) for tree in self.trees])\n",
        "  tree_predict = np.swapaxes(tree_predict,0,1) #each of the trees will give out predictions\n",
        "  y_pred = [most_common_label(tree_pred) for tree_pred in tree_predict] \n",
        "  return np.array(y_pred) \n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "GcsHmtb_zbaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(y_true, y_pred):\n",
        "  accuracy = np.sum(y_true == y_pred)/len(y_true) \n",
        "  return accuracy "
      ],
      "metadata": {
        "id": "_eSnVGKs20qw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def GradBoost(model, X_train, y_train, X_test, boosting_rounds, learning_rate: float = 0.1):\n",
        "  # make a first guess of our training target variable using the mean of y_train\n",
        "  y_hat_train = np.repeat(np.mean(y_train), len(y_train))\n",
        "\n",
        "  # initialize the test prediction with the mean of the training target variable\n",
        "  y_hat_test = np.repeat(np.mean(y_train), len(X_test)) \n",
        "\n",
        "  #calculate the residuals from the training data using the first guess\n",
        "  residuals = y_train - y_hat_train \n",
        "\n",
        "  #iterates through the boosting round.\n",
        "  for i in range(0,boosting_rounds):\n",
        "    #fit the model to the residuals\n",
        "    model = model.fit(X_train, residuals) \n",
        "\n",
        "    #increment the predicted training y with the pseudo residual * learning rate\n",
        "    y_hat_train = y_hat_train + learning_rate*model.predict(X_train) \n",
        "\n",
        "    #increment the predicted test y as well\n",
        "    y_hat_test = y_hat_test + learning_rate * model.predict(X_test) \n",
        "\n",
        "    #calculate the residuals for the next round \n",
        "\n",
        "    residuals = y_train - y_hat_train \n",
        "  return y_hat_train, y_hat_test"
      ],
      "metadata": {
        "id": "_dnZJw8SmfUg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "###Week11: (Clustering)"
      ],
      "metadata": {
        "id": "VySUeU2AzbpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(X,k):\n",
        "  # take three random points from data set and take them as initial centroids\n",
        "\n",
        "  centroids = X[np.random.randint(0,X.shape[0],size=k)] \n",
        "\n",
        "  #calculate initial label of each data point\n",
        "  labels = np.argmin(cdist(X,centroids), axis=1)\n",
        "\n",
        "  for _iteration in range(10):\n",
        "    #copy labels for all points for comparing later\n",
        "    previous_labels =labels.copy()\n",
        "\n",
        "    #compute new centroids\n",
        "    centroids = np.array([np.mean(X[labels==r], axis=0) for r in range(k)])  \n",
        "\n",
        "    #at last recalculate label of each data point \n",
        "    labels = np.argmin(cdist(X,centroids),axis=1) \n",
        "\n",
        "    #check if labels of points are not cchanging\n",
        "    if all (labels ==previous_labels):\n",
        "      break\n",
        "  return centroids, labels"
      ],
      "metadata": {
        "id": "U1mwiQwnzbpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_kmeans(X,centroids, labels, rseed=0, ax=None):\n",
        "  #plot input data\n",
        "\n",
        "  ax = ax or plt.gca() \n",
        "  ax.axis('equal') \n",
        "\n",
        "  #plot data points\n",
        "\n",
        "  ax.scatter(X[:,0],X[:,1],c=labels, s=40, cmap='plasma', zorder=2)\n",
        "\n",
        "  #plot the representation of the KMeans model.\n",
        "\n",
        "  radii = [cdist(X[labels==i],[center]).max() for i, center in enumerate(centroids)] \n",
        "\n",
        "  for c,r in zip(centroids, radii):\n",
        "    #add circular shapes to the clusters\n",
        "    ax.add_patch(plt.Circle(c,r, fc='#CCCCCC',lw =5, alpha=0.5,zorder=1))               "
      ],
      "metadata": {
        "id": "KUYUntJDFTZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Week2:"
      ],
      "metadata": {
        "id": "TOU1rVDuooc5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "6guR0LhMJ3E_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit(X,y):\n",
        "  return np.linalg.pinv(X)@y\n",
        "def loss(X,y,w):\n",
        "  e = X@w -y\n",
        "  return 1/2 * e.T@e \n",
        "\n",
        "def calculate_gradient(X,y,w):\n",
        "  return X.T@(X@w-y)\n",
        "\n",
        "def update_weights(w,grad,lr):\n",
        "  return (w - lr*grad)\n",
        "\n",
        "\n",
        "def gradient_descent(X,y,lr,num_epochs):\n",
        "  w_all = [] \n",
        "  err_all = []\n",
        "  \n",
        "  w = np.zeros((X.shape[1])) \n",
        "  for i in np.arange(0,num_epochs): \n",
        "    w_all.append(w) \n",
        "    err_all.append(loss(X,y,w))\n",
        "    grad = calculate_gradient(X,y,w) \n",
        "\n",
        "    if (i%100==0):\n",
        "      print('Iteration {0}#, loss {1:.2f} : '.format(i,err_all[-1])) \n",
        "      w = update_weights(w,grad,lr) \n",
        "  return w, err_all, w_all \n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "def generate_data(n=1000_000):\n",
        "  w=np.array([4,3])\n",
        "  X=10*np.random.rand(n)\n",
        "  X=add_dummy_feature(X)\n",
        "  noise=np.random.rand(n)\n",
        "  y= X@w + noise\n",
        "  return X,y \n",
        "\n",
        "def preprocess(X,y):\n",
        "  X_train,y_train,X_test,y_test = train_test_split(X,y, test_size=0.2, random_state=42) \n",
        "  return X_train, y_train,X_test,y_test\n",
        "\n",
        "def add_dummy_feature(X):\n",
        "  return np.column_stack((np.ones(X.shape[0]),X))\n",
        "\n",
        "def predict(X,w):\n",
        "  return X@w\n",
        "\n",
        "def plot_learning_curves(err_all):\n",
        "  plt.plot(err_all)\n",
        "  plt.xlabel('Iterations #') \n",
        "  plt.ylabel('Loss: $J(\\mathbf{w})$')\n",
        "\n",
        "def learning_schedule(t):\n",
        "  t0,t1 = 200,100000 \n",
        "  return t0/(t1+t) \n",
        "\n",
        "def mbgd(X,y,num_epochs,batch_size):\n",
        "  w_all =[]\n",
        "  err_all =[] \n",
        "  w = np.zeros((X.shape[1])) \n",
        "  t = 0 \n",
        "  for epoch in range(num_epochs):\n",
        "    shuffled_indices = np.random.permutation(X.shape[0]) \n",
        "    X_shuffled = X[shuffled_indices] \n",
        "    y_shuffled = y[shuffled_indices] \n",
        "\n",
        "    for i in range(0,X.shape[0],batch_size):\n",
        "      t+=1\n",
        "      xi= X_shuffled[i:i+batch_size]\n",
        "      yi= y_shuffled[i:i+batch_size]\n",
        "\n",
        "      err_all.append(loss(xi,yi,w))\n",
        "\n",
        "      grad = 2/batch_size * calculate_gradient(xi,yi,w)\n",
        "      lr = learning_schedule(t) \n",
        "      w = update_weights(w,grad,lr) \n",
        "      w_all.append(w) \n",
        "  return w ,err_all, w_all \n",
        "\n",
        "def sgd(X,y,num_epochs):\n",
        "  w = np.zeros((X.shape[1])) \n",
        "  w_all = []\n",
        "  err_all =[] \n",
        "    \n",
        "  for epoch in range(num_epochs):\n",
        "    for i in range(X.shape[0]):\n",
        "      random_index = np.random.randint((X.shape[0])) \n",
        "      xi = X[random_index:random_index+1]\n",
        "      yi = y[random_index:random_index+1] \n",
        "      err_all.append(loss(xi,yi,w))\n",
        "      w_all.append(w)\n",
        "      grad = 2* calculate_gradient(xi,yi,w)\n",
        "      lr = learning_schedule(epoch*X.shape[0]+i)\n",
        "      w = update_weights(w,grad,lr) \n",
        "\n",
        "  return w, err_all,w_all\n",
        "\n",
        "      "
      ],
      "metadata": {
        "id": "fEaxPLOr6sEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LinReg():\n",
        "    def __init__(self):\n",
        "        self.t0 = 200\n",
        "        self.t1 = 100000\n",
        "    def predict (self, X):\n",
        "        y = X @ self.w\n",
        "        return y\n",
        "    def loss (self, X, y):\n",
        "        e = y - self.predict(X)\n",
        "        return 0.5 *(e.T @ e)\n",
        "    def rmse(self,X, y):\n",
        "        return np.sqrt(2/X.shape[0] * self.loss(X, y))\n",
        "    def fit(self, X, y):\n",
        "        self.w = np.linalg.pinv(X) @ y\n",
        "        return self.w\n",
        "    def calculate_gradient(self, X, y):\n",
        "        return X.T @ (self.predict(X) - y)\n",
        "    def update_weights(self, grad, lr):\n",
        "        return (self.w - lr * grad)\n",
        "    def learning_schedule(self, t):\n",
        "        return self.t0 / (self.t0 + self.t1)\n",
        "    def gd(self, X, y, num_epochs, lr):\n",
        "        self.w = np.zeros(X.shape[1])\n",
        "        self.w_all = list() \n",
        "        self.err_all = list()\n",
        "        for i in range(epochs):\n",
        "            dJdw = calculate_gradient(X, y)\n",
        "            self.w_all.append(self.w)\n",
        "            self.err_all.append(self.loss(X, y))\n",
        "            self.w = self.update_weights(dJdw, lr)\n",
        "        return self.w\n",
        "    def mbgd(self, X, y, num_epochs, batch_size):\n",
        "        mini_batch_id = 0\n",
        "        self.w = np.zeros(X.shape[1])  #initializing arbitrary values.\n",
        "        self.w_all = list() \n",
        "        self.err_all = list()\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            shuffled_indices = np.random.permutation(X.shape[0])\n",
        "            X_shuffled = X[shuffled_indices]\n",
        "            y_shuffled = y[shuffled_indices]\n",
        "\n",
        "            for i in range(0, X.shape[0], batch_size):\n",
        "                mini_batch_id += 1\n",
        "                x1 = X_shuffled[i:i+batch_size]\n",
        "                y1 = y_shuffled[i:i+batch_size]\n",
        "\n",
        "                self.w_all.append(self.w)\n",
        "                self.err_all.append(self.loss(X, y))\n",
        "\n",
        "                dJdw = 2/batch_size * self.calculate_gradient(x1, y1)\n",
        "                self.w = self.update_weights(dJdw, self.learning_schedule(mini_batch_id))\n",
        "\n",
        "        return self.w\n",
        "    \n",
        "    def sgd(self, X, y, num_epochs):\n",
        "        batch_size = 1\n",
        "        mini_batch_id = 0\n",
        "        self.w = np.zeros(X.shape[1])  #initializing arbitrary values.\n",
        "        self.w_all = list() \n",
        "        self.err_all = list()\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            shuffled_indices = np.random.permutation(X.shape[0])\n",
        "            X_shuffled = X[shuffled_indices]\n",
        "            y_shuffled = y[shuffled_indices]\n",
        "\n",
        "            for i in range(0, X.shape[0], batch_size):\n",
        "                mini_batch_id += 1\n",
        "                x1 = X_shuffled[i:i+batch_size]\n",
        "                y1 = y_shuffled[i:i+batch_size]\n",
        "\n",
        "                self.w_all.append(self.w)\n",
        "                self.err_all.append(self.loss(X, y))\n",
        "\n",
        "                dJdw = 2/batch_size * self.calculate_gradient(x1, y1)\n",
        "                self.w = self.update_weights(dJdw, self.learning_schedule(mini_batch_id))\n",
        "\n",
        "        return self.w"
      ],
      "metadata": {
        "id": "KOa5o6g5Js_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Week3:"
      ],
      "metadata": {
        "id": "uROKA5gHoxYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinReg():\n",
        "    def __init__(self):\n",
        "        self.t0 = 200\n",
        "        self.t1 = 100000\n",
        "    def predict (self, X):\n",
        "        y = X @ self.w\n",
        "        return y\n",
        "    def loss (self, X, y, reg_rate):\n",
        "        e = y - self.predict(X)\n",
        "        return 0.5 *(e.T @ e) + (reg_rate/2) * (np.transpose(self.w) @ self.w)\n",
        "    def rmse(self,X, y, reg_rate):\n",
        "        return np.sqrt(2/X.shape[0] * self.loss(X, y, reg_rate))\n",
        "    def fit(self, X, y, reg_rate):\n",
        "#         self.w = np.linalg.pinv(X) @ y\n",
        "        eye = np.eye(X.shape[1])\n",
        "        self.w = np.linalg.solve(X.T @ X + reg_rate * eye, X.T @ y)\n",
        "        return self.w\n",
        "    def calculate_gradient(self, X, y, reg_rate):\n",
        "        return X.T @ (self.predict(X) - y) + reg_rate * self.w\n",
        "    def update_weights(self, grad, lr):\n",
        "        return (self.w - lr * grad)\n",
        "    def learning_schedule(self, t):\n",
        "        return self.t0 / (self.t0 + self.t1)\n",
        "    def gd(self, X, y, num_epochs, lr, reg_rate):\n",
        "        self.w = np.zeros(X.shape[1])\n",
        "        self.w_all = list() \n",
        "        self.err_all = list()\n",
        "        for i in range(num_epochs):\n",
        "            dJdw = self.calculate_gradient(X, y, reg_rate)\n",
        "            self.w_all.append(self.w)\n",
        "            self.err_all.append(self.loss(X, y, reg_rate))\n",
        "            self.w = self.update_weights(dJdw, lr)\n",
        "        return self.w\n",
        "    def mbgd(self, X, y, num_epochs, batch_size):\n",
        "        mini_batch_id = 0\n",
        "        self.w = np.zeros(X.shape[1])  #initializing arbitrary values.\n",
        "        self.w_all = list() \n",
        "        self.err_all = list()\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            shuffled_indices = np.random.permutation(X.shape[0])\n",
        "            X_shuffled = X[shuffled_indices]\n",
        "            y_shuffled = y[shuffled_indices]\n",
        "\n",
        "            for i in range(0, X.shape[0], batch_size):\n",
        "                mini_batch_id += 1\n",
        "                x1 = X_shuffled[i:i+batch_size]\n",
        "                y1 = y_shuffled[i:i+batch_size]\n",
        "\n",
        "                self.w_all.append(self.w)\n",
        "                self.err_all.append(self.loss(X, y))\n",
        "\n",
        "                dJdw = 2/batch_size * self.calculate_gradient(x1, y1)\n",
        "                self.w = self.update_weights(dJdw, self.learning_schedule(mini_batch_id))\n",
        "\n",
        "        return self.w\n",
        "    \n",
        "    def sgd(self, X, y, num_epochs):\n",
        "        batch_size = 1\n",
        "        mini_batch_id = 0\n",
        "        self.w = np.zeros(X.shape[1])  #initializing arbitrary values.\n",
        "        self.w_all = list() \n",
        "        self.err_all = list()\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            shuffled_indices = np.random.permutation(X.shape[0])\n",
        "            X_shuffled = X[shuffled_indices]\n",
        "            y_shuffled = y[shuffled_indices]\n",
        "\n",
        "            for i in range(0, X.shape[0], batch_size):\n",
        "              mini_batch_id += 1\n",
        "                x1 = X_shuffled[i:i+batch_size]\n",
        "                y1 = y_shuffled[i:i+batch_size]\n",
        "\n",
        "                self.w_all.append(self.w)\n",
        "                self.err_all.append(self.loss(X, y))\n",
        "\n",
        "                dJdw = 2/batch_size * self.calculate_gradient(x1, y1)\n",
        "                self.w = self.update_weights(dJdw, self.learning_schedule(mini_batch_id))\n",
        "\n",
        "        return self.w"
      ],
      "metadata": {
        "id": "Tp5bTS8KozWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Week 4"
      ],
      "metadata": {
        "id": "o5qNrfyBCqfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def combinations(x, degree):\n",
        "    return itertools.combinations_with_replacement(x, degree)"
      ],
      "metadata": {
        "id": "MVWeiW4ugBkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_new_features(items):\n",
        "    return functools.reduce(lambda x, y: x * y, items)"
      ],
      "metadata": {
        "id": "vG9jAm3BgFPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def polynomial_transform(x, degree):\n",
        "    if x.ndim == 1:\n",
        "        x = x[:, None]\n",
        "\n",
        "    features = [np.ones(len(x))] #a list of np-arrays with all 1.\n",
        "\n",
        "    for degree in range(1, degree + 1):\n",
        "        for item in combinations(x.T, degree):\n",
        "            features.append(compute_new_features(item))\n",
        "            \n",
        "    return np.asarray(features).T"
      ],
      "metadata": {
        "id": "8IJ3XY2sgH7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(arr):\n",
        "    encoded = np.zeros((arr.size, arr.max() + 1))\n",
        "    encoded[np.arange(arr.size), arr] = 1\n",
        "    return encoded"
      ],
      "metadata": {
        "id": "c7k_grr4hK-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_learning_curve(err_all):\n",
        "    err = [err[1][1] for err in err_all]\n",
        "    plt.plot(np.arange(len(err)), err, 'r-')"
      ],
      "metadata": {
        "id": "vLdkQ-dph1-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(add_class=False, add_outliers=False, degree=1):\n",
        "    x, y = create_toy_data(add_outliers, add_class)\n",
        "    x_poly = polynomial_transform(x, degree=degree)\n",
        "    x_train, x_test, y_train, y_test = train_test_split(x_poly, y) \n",
        "    y_train_trans = LabelTransformer().encode(y_train)\n",
        "    y_test_trans = LabelTransformer().encode(y_test)\n",
        "    return x_train , x_test, y_train, y_test, y_train_trans, y_test_trans "
      ],
      "metadata": {
        "id": "WI94F2eegIwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_toy_data(add_outliers=False, add_class=False):\n",
        "    x0 = np.random.normal(size=50).reshape(-1,2) - 1\n",
        "    x1 = np.random.normal(size=50).reshape(-1,2) + 1\n",
        "    if add_outliers:  # add 5 (pairs of (x, y)) outliers\n",
        "        x_1 = np.random.normal(size=10).reshape(-1,2) + np.array([5., 10.])\n",
        "        return np.concatenate((x0, x1, x_1)), np.concatenate((np.zeros(25), np.ones(30))).astype(int)\n",
        "    if add_class: #add 25 (pairs of (x, y) additional data)\n",
        "        x2 = np.random.normal(size=50).reshape(-1,2) + 2\n",
        "        return np.concatenate((x0, x1, x2)), np.concatenate((np.zeros(25), np.ones(25), 2 + np.zeros(25))).astype(int)\n",
        "    return np.concatenate((x0, x1)), np.concatenate((np.zeros(25), np.ones(25))).astype(int)"
      ],
      "metadata": {
        "id": "iGRw2AgAhnuV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.set_context(context='notebook',font_scale=1.5,rc={\"lines.linewidth\":2.5})\n",
        "\n",
        "def visualize_model(X_train, labels, lsc, degree=1):\n",
        "#     sns.set_style(\"white\")\n",
        "    f = plt.figure(figsize=(8,8))\n",
        "\n",
        "    x1_min = np.min(x_train[:,1])\n",
        "    x1_max = np.max(x_train[:,1])\n",
        "    x2_min = np.min(x_train[:,2])\n",
        "    x2_max = np.max(x_train[:,2])\n",
        "    \n",
        "    x1_test, x2_test = np.meshgrid(np.linspace(x1_min, x1_max, 100), np.linspace(x2_min, x2_max, 100)) \n",
        "    x_test = np.array([x1_test, x2_test]).reshape(2,-1).T\n",
        "    x_test_poly = polynomial_transform(x_test, degree=degree)\n",
        "    y_test = lsc.predict(x_test_poly)\n",
        "    \n",
        "    sns.scatterplot(data=x_train, x=x_train[:,1], y=x_train[:,2], hue=labels)\n",
        "    plt.contourf(x1_test, x2_test, y_test.reshape(100,100), alpha=0.5, levels=np.linspace(0,1,3))\n",
        "    plt.gca().set_aspect('equal', adjustable='box')"
      ],
      "metadata": {
        "id": "tjis3hEchsWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LeastSquareClassification(object):\n",
        "  def __init__(self):\n",
        "    self.t0=20\n",
        "    self.t1=1000\n",
        "  \n",
        "  def predict(self,X):\n",
        "    return np.argmax(X @ self.w, axis=-1)\n",
        "\n",
        "  def predict_internal(self,X):\n",
        "    return X @ self.w\n",
        "  \n",
        "  def loss(self,X,y,reg_rate):\n",
        "    y_hat = self.predict_internal(X)\n",
        "    err = y_hat - y\n",
        "    return (1/2) * (err.T @ err) + (reg_rate / 2) * (self.w.T @ self.w)\n",
        "\n",
        "  def fit(self,X, y, reg_rate=0):\n",
        "    self.w = np.linalg.solve(X.T @ X + reg_rate * np.eye(X.shape[-1]), X.T@y)\n",
        "    print(X.shape, y.shape, self.w.shape)\n",
        "    return self.w\n",
        "  \n",
        "  def calculate_gradient(self, X, y, reg_rate):\n",
        "    y_hat = self.predict_internal(X)\n",
        "    return X.T @ (y_hat - y) + reg_rate * self.w\n",
        "  \n",
        "  def weight_updates(self, grad,lr):\n",
        "    return (self.w - lr*grad)\n",
        "  \n",
        "  def learning_schedule(self, t):\n",
        "    return self.t0 / (t + self.t1)\n",
        "  \n",
        "  def gd(self, X, y, num_epochs, lr, reg_rate):\n",
        "    self.w = np.zeros((X.shape[-1], y.shape[-1]))\n",
        "    self.w_all = []\n",
        "    self.err_all = []\n",
        "    for i in np.arange(0, num_epochs):\n",
        "      djdw = self.calculate_gradient(X, y, reg_rate)\n",
        "      self.w_all.append(self.w)\n",
        "      self.err_all.append(self.loss(X, y, reg_rate))\n",
        "      self.w = self.weight_updates(djdw, lr)\n",
        "    return self.w\n",
        "  \n",
        "  def sgd(self, X, y, num_epochs, reg_rate):\n",
        "    self.err_all = []\n",
        "    self.w_all = []\n",
        "    self.w=np.zeros((X.shape[-1], y.shape[-1]))\n",
        "    t=0\n",
        "    for epoch in range(num_epochs):\n",
        "      for iter in range(X.shape[0]):\n",
        "        t = t+1\n",
        "        random_index = np.random.randint(X.shape[0])\n",
        "        x1 = X[random_index:random_index+1]\n",
        "        y1 = y[random_index:random_index+1]\n",
        "\n",
        "        self.w_all.append(self.w)\n",
        "        self.err_all.append(self.loss(x1, y1, reg_rate))\n",
        "\n",
        "        djdw = self.calculate_gradient(x1, y1, reg_rate)\n",
        "        self.w = self.weight_updates(djdw, self.learning_schedule(t))\n",
        "    return self.w"
      ],
      "metadata": {
        "id": "mCQGGixKgyDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_hat = lsc.predict(x_test)\n",
        "\n",
        "tp = np.where((y_test == 1) & (y_test_hat == 1), 1, 0).sum()\n",
        "tn = np.where((y_test == 0) & (y_test_hat == 0), 1, 0).sum()\n",
        "fp = np.where((y_test == 0) & (y_test_hat == 1), 1, 0).sum()\n",
        "fn = np.where((y_test == 1) & (y_test_hat == 0), 1, 0).sum()\n",
        "\n",
        "def precision(tp, fp):\n",
        "  if (tp + fp) == 0:\n",
        "    return NaN\n",
        "  return tp / (tp + fp)\n",
        "def recall(tp,fn):\n",
        "  if (tp+fn) ==0:\n",
        "    return NaN\n",
        "  return tp / tp +fn\n",
        "def f1_score(pr,re):\n",
        "  return 2 * ((pr * re)/(pr + re))\n",
        "def accuracy(tp,fp,fn,tn):\n",
        "  return (tp + tn) / (tp + tn + fp + fn)\n"
      ],
      "metadata": {
        "id": "xsVf8ZIirXFu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}