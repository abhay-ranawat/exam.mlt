{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps\n",
    "1. Make a first guess for y_train and y_test, using average of y_train\n",
    "$$\n",
    "y_{train_{p_0}} = \\frac{1}{n} \\sum_{i=1}^n y_{train_{i}}  \\\\\n",
    "y_{test_{p_0}} = y_{train_{p_0}} \n",
    "$$\n",
    "2. Calculate the residuals from the training set\n",
    "$$\n",
    "r_0 = y_{train} - y_{train_{p_0}}\n",
    "$$\n",
    "3. Fit a week learner to the residuals minimizing the loss function. Let's call it $f_0$.\n",
    "$$\n",
    "r_0 = f_0(X_{train})\n",
    "$$\n",
    "4. Increment the predicted y's.\n",
    "$$\n",
    "y_{train_{p_1}} = y_{train_{p_0}} + \\alpha f_0(X_{train}) \\\\\n",
    "y_{test_{p_1}} = y_{test_{p_0}} + \\alpha f_0(X_{test}) \\\\\n",
    "$$\n",
    "$\\alpha$ is the learning rate.\n",
    "\n",
    "5. Repeat 2 to 4 until you reach the number of boosting rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GradBoost(model, X_train, y_train, X_test, boosting_rounds, learning_rate: float = 0.1):\n",
    "    #make a first guess of our training target variable using the mean of y_train\n",
    "    y_hat_train = np.repeat(np.mean(y_train), len(y_train))\n",
    "\n",
    "    #Initiate the test prediction with the mean of the training target variable.\n",
    "    y_hat_test = np.repeat(np.mean(y_train), len(y_test))\n",
    "\n",
    "    #Calculate the residuals\n",
    "    residuals = y_train - y_hat_train\n",
    "\n",
    "    #Iterate through the boosting rounds\n",
    "    for i in range(boosting_rounds):\n",
    "        #Fit the model to the residuals\n",
    "        model = model.fit(X_train, residuals)\n",
    "\n",
    "        #Increment the predicted training y with the pseudoresidulas * learning rate\n",
    "        y_hat_train = y_hat_train + learning_rate * model.predict(X_train)\n",
    "\n",
    "        #Increment the predicted test y as well\n",
    "        y_hat_test = y_hat_test + learning_rate * model.predict(X_test)\n",
    "\n",
    "        #Calculate the residuals for the next round\n",
    "        residuals = y_train - y_hat_train\n",
    "\n",
    "    return y_hat_train, y_hat_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y = make_regression(n_samples = 1000,\n",
    "                        n_features = 20,\n",
    "                        n_informative = 15,\n",
    "                        n_targets = 1,\n",
    "                        bias = 0.0,\n",
    "                        noise = 20,\n",
    "                        shuffle = True,\n",
    "                        random_state = 13)\n",
    "\n",
    "X_train = X[0:800]\n",
    "y_train = y[0:800]\n",
    "\n",
    "X_test = X[800:]\n",
    "y_test = y[800:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "model = DecisionTreeRegressor(criterion='squared_error', max_depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_train = []\n",
    "n_rounds = np.arange(5, 101, 5)\n",
    "for n_round in n_rounds:\n",
    "    y_hat_train = GradBoost(model,\n",
    "                            X_train,\n",
    "                            y_train,\n",
    "                            X_test,\n",
    "                            boosting_rounds=n_round,\n",
    "                            learning_rate=0.1)[0]\n",
    "\n",
    "    mse_train.append(np.mean((y_train - y_hat_train) **2))\n",
    "    print(\"round #: {0}, mse: {1}\".format(n_round, mse_train[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(n_rounds, mse_train)\n",
    "plt.title(\"Training MSE vs Boosting Rounds\", fontsize=20)\n",
    "plt.xlabel('NUmber of Boosting rounds', fontsize=15)\n",
    "plt.ylabel('Training Mean squared error', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit ('3.8.13')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "110cc1dee26208153f2972f08a2ad52b6a56238dc66d48e87fb757ef2996db56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
