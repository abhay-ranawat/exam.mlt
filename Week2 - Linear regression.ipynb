{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression\n",
    "\n",
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from Ipython.display import display, Math, Latex\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data set of 100 examples and pne feature\n",
    "def generate_data(n):\n",
    "    \n",
    "    w0 = 4.0\n",
    "    w1 = 3.0\n",
    "\n",
    "    X = 10*np.random.rand(n,)\n",
    "\n",
    "    y = w0 + w1*X + np.random.randn(n,)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "X, y = generate_data(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"shape of training feature matrix: \", X.shape)\n",
    "print(\"shape of label matrix: \", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's divide this into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Sanity Check\n",
    "print(\"shape of training feature matrix: \", X_train.shape)\n",
    "print(\"shape of training label matrix: \", y_train.shape)\n",
    "print(\"shape of test feature matrix: \", X_test.shape)\n",
    "print(\"shape of test label matrix: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make plots\n",
    "\n",
    "def make_plot(x,y):\n",
    "    '''\n",
    "        Function to make plot\n",
    "\n",
    "        Args:\n",
    "            x: x values\n",
    "            y: y values\n",
    "        \n",
    "        Returns:\n",
    "            plot of x vs y\n",
    "    '''\n",
    "    sns.set_style(\"white\")\n",
    "    f = plt.figure(figsize=(8,8))\n",
    "    sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n",
    "\n",
    "    plt.plot(X_train, y_train, \"b.\")\n",
    "    plt.title(\"Data Points\")\n",
    "    plt.grid(True)\n",
    "    plt.xlabel(\"$X_1$\", fontsize=18)\n",
    "    plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "    plt.axis([0,10,0,40])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualise\n",
    "\n",
    "make_plot(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to add dummy feature to our feature matrix\n",
    "\n",
    "def add_dummy_feature(x):\n",
    "    '''   \n",
    "    Adds dummy feature to the dataset.\n",
    "\n",
    "        Args:\n",
    "            x: Training dataset\n",
    "\n",
    "        Returns:\n",
    "            Training dataset with an addition of dummy feature\n",
    "\n",
    "    '''\n",
    "    return np.column_stack((np.ones(x.shape[0]), x))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "\n",
    "import unittest\n",
    "\n",
    "class TestAddDummyFeature(unittest.TestCase):\n",
    "\n",
    "    def test_add_dummy_feature(self):\n",
    "        ''' Test case function for add_dummy_feature'''\n",
    "\n",
    "        train_matrix = np.array([[3,2,5],[9,4,7]])\n",
    "        train_matrix_with_dummy_feature = add_dummy_feature(train_matrix)\n",
    "\n",
    "        #test the shape\n",
    "        self.assertEqual(train_matrix_with_dummy_feature.shape, (2,4))\n",
    "\n",
    "        # and contents\n",
    "        np.testing.assert_array_equal(train_matrix_with_dummy_feature,np.array([[1,3,2,5],[1,9,4,7]]))\n",
    "         \n",
    "unittest.main(argv=[''], defaultTest='TestAddDummyFeature', verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add dummy feature to training set\n",
    "print(\"Before adding dummy feature:\\n\", X_train[:5])\n",
    "print(\"\\n\")\n",
    "\n",
    "X_train_with_dummy = add_dummy_feature(X_train)\n",
    "\n",
    "print(\"After adding dummy feature:\\n\", X_train_with_dummy[:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X,w):\n",
    "    '''\n",
    "    Prediction of ouotput label for a given input\n",
    "\n",
    "    Args:\n",
    "        X: Feature matrix of shape (n, m+1).\n",
    "        w: weight vector of shape (m+1,)\n",
    "\n",
    "    Returns:\n",
    "        y: Predicted label vector of shape (n,) \n",
    "\n",
    "    '''\n",
    "    #check to make sure the feature matrix and weight vector have compatible shapes\n",
    "    assert X.shape[-1] == w.shape[0], \"X and w don't have compatible dimensions\"\n",
    "    return X @ w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's test the function\n",
    "import unittest\n",
    "\n",
    "class TestPredict(unittest.TestCase):\n",
    "\n",
    "    def test_predict(self):\n",
    "        '''Test case predict function of linear regression'''\n",
    "        # Set up\n",
    "        train_matrix = np.array([[1,3,2,5],[1,9,4,7]])\n",
    "        weight_vector = np.array([1,1,1,1])\n",
    "        expected_label_vector = np.array([11,21])\n",
    "\n",
    "        #call\n",
    "        predicted_label_vector = predict(train_matrix, weight_vector)\n",
    "\n",
    "        #asserts\n",
    "        #test the shape\n",
    "        self.assertEqual(predicted_label_vector.shape, (2,))\n",
    "\n",
    "        # and the contents\n",
    "        np.testing.assert_array_equal(expected_label_vector, predicted_label_vector)\n",
    "\n",
    "unittest.main(argv=[''], defaultTest='TestPredict', verbosity=2, exit=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstration on synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly initialise weight vector \n",
    "\n",
    "w = np.random.rand(2,)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted labels\n",
    "\n",
    "y_hat = predict(X_train_with_dummy, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(X, y, w):\n",
    "    '''Calculates loss for a model based on known labels\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix for given inputs\n",
    "        y: Output label vector as predicted by the given model\n",
    "        w: Weight vector\n",
    "\n",
    "    Returns:\n",
    "        Loss\n",
    "    '''\n",
    "\n",
    "    err = predict(X,w) - y\n",
    "\n",
    "    return (1/2)*(np.transpose(err) @ err)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test cases\n",
    "\n",
    "class TestLossFunction((unittest.TestCase)):\n",
    "    \n",
    "    def test_loss_function(self):\n",
    "        '''Test case for loss function of linear regression'''\n",
    "\n",
    "        # Set up\n",
    "        feature_matrix = np.array([[1,3,2,5],[1,9,4,7]])\n",
    "        weight_vector = np.array([1,1,1,1])\n",
    "        label_vector = np.array([6,11])\n",
    "        expected_loss = np.array([62.5])\n",
    "\n",
    "        # call\n",
    "        loss_value = loss(feature_matrix, label_vector, weight_vector)\n",
    "\n",
    "        #asserts\n",
    "        #test the shape\n",
    "        self.assertEqual(loss_value.shape, ())\n",
    "\n",
    "        #and contents\n",
    "        np.testing.assert_array_equal(expected_loss, loss_value)\n",
    "\n",
    "unittest.main(argv=[''], defaultTest = 'TestLossFunction', verbosity = 2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's generate some data\n",
    "\n",
    "X, y = generate_data(100)\n",
    "\n",
    "# Function to preprocess, Add dummy feature, test-train split\n",
    "\n",
    "def preprocess(x, y):\n",
    "\n",
    "    X = add_dummy_feature(x)\n",
    "\n",
    "    return train_test_split(X, y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = preprocess(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normal_equation(X,y):\n",
    "    '''Estimates parameters of the linear regression model with normal equation.\n",
    "    \n",
    "    Args: \n",
    "        X: Feature matrix for given inputs.\n",
    "        y: Actual label vector.\n",
    "\n",
    "    Returns:\n",
    "        Weight vector\n",
    "    '''\n",
    "\n",
    "    return np.linalg.pinv(X) @ y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "class TestNormalEquation(unittest.TestCase):\n",
    "    \n",
    "    def test_normal_equation(self):\n",
    "        '''\n",
    "            Test case for weeight estimation for linear regression with Normal equation method\n",
    "        ''' \n",
    "\n",
    "        #Set up\n",
    "\n",
    "        feature_matrix = X_train\n",
    "        label_vector = y_train\n",
    "        expected_weight_vector = np.array([4., 3.])\n",
    "\n",
    "        #call\n",
    "        estimated_weight_vector = normal_equation(feature_matrix, label_vector)\n",
    "\n",
    "        #asserts\n",
    "        #test the shape\n",
    "        self.assertEqual(estimated_weight_vector.shape, (2,))\n",
    "\n",
    "        #and contents\n",
    "        np.testing.assert_array_almost_equal(estimated_weight_vector, expected_weight_vector, decimal=0)\n",
    "\n",
    "unittest.main(argv=[''], defaultTest='TestNormalEquation', verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent(GD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradient(X, y, w):\n",
    "    '''Calculates gradients of loss dunction wrt weight vector on training set\n",
    "    \n",
    "    Arguments:\n",
    "\n",
    "        X: Feature matrix for training data.\n",
    "        y: Label vector for training data\n",
    "        w: Weight vector\n",
    "\n",
    "    Returns:\n",
    "        A vector of gradients.\n",
    "    '''\n",
    "\n",
    "    return np.transpose(X) @ (predict(X, w) - y)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestCalculateGradient(unittest.TestCase):\n",
    "\n",
    "    def test_calculate_gradient(self):\n",
    "\n",
    "        '''\n",
    "        Test case for gradient descent\n",
    "        '''\n",
    "\n",
    "        #Set up\n",
    "        feature_matrix = np.array([[1,3,2,5],[1,9,4,7]])\n",
    "        weight_vector = np.array([1,1,1,1])\n",
    "        label_vector = np.array([6,11])\n",
    "        expected_grad = np.array([15,105,50,95])\n",
    "\n",
    "        #Call\n",
    "        grad = calculate_gradient(feature_matrix, label_vector, weight_vector)\n",
    "\n",
    "        #asserts\n",
    "        #test the shape\n",
    "        self.assertEqual(grad.shape, (4, ))\n",
    "\n",
    "        #and contents\n",
    "        np.testing.assert_array_almost_equal(expected_grad, grad, decimal=0)\n",
    "\n",
    "unittest.main(argv=[''], defaultTest='TestCalculateGradient', verbosity=2,exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weight Updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(w, grad, lr):\n",
    "    '''\n",
    "    Updates the weights based on the gradient of loss function.\n",
    "\n",
    "    Weight updates are carried out with the fiollowing formula:\n",
    "        w_new := w_old - lr * grad\n",
    "\n",
    "    Args:\n",
    "        1. w: Weight vector\n",
    "        2. grad: gradient of loss w.r.t w\n",
    "        3. lr: learning rate\n",
    "\n",
    "    Returns:\n",
    "        Updated weight vector\n",
    "    '''\n",
    "\n",
    "    return (w - lr*grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestUpdateWeights(unittest.TestCase):\n",
    "\n",
    "    def test_update_weight(self):\n",
    "\n",
    "        #setup\n",
    "        weight_vector = np.array([1,1,1,1])\n",
    "        grad_vector = np.array([15, 105, 50, 95])\n",
    "        lr = 0.001\n",
    "        expected_w_new = np.array([0.985, 0.895, 0.95, 0.905])\n",
    "\n",
    "        #call\n",
    "        w_new = update_weights(weight_vector, grad_vector, lr)\n",
    "\n",
    "        #asserts\n",
    "        # test the shape\n",
    "        self.assertEqual(expected_w_new.shape, (4, ))\n",
    "\n",
    "        #and contents\n",
    "        np.testing.assert_array_almost_equal(expected_w_new, w_new, decimal=1)\n",
    "\n",
    "unittest.main(argv=[''], defaultTest='TestUpdateWeights', verbosity=2, exit=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X:np.ndarray, y:np.ndarray, lr:float, num_epochs:int):\n",
    "    '''Estimates parameters of linear regression model through gradient descent.\n",
    "    \n",
    "    Arguments:\n",
    "        X: Feature matrix for training data\n",
    "        y: Label vector for training data\n",
    "        lr: learning rate\n",
    "        num_epochs: Number of training steps\n",
    "\n",
    "    Returns:\n",
    "        Weight vector: Final weight vector\n",
    "        Error vector across different iterations\n",
    "        Weight vectors acrss different iterations\n",
    "    '''\n",
    "\n",
    "    w_all = [] #all paramenter across iterations\n",
    "    err_all = [] # all arrors across iterations\n",
    "\n",
    "    # parameter vector initialised to zero\n",
    "    w = np.zeros((X.shape[1]))\n",
    "\n",
    "    #Gradient descent loop\n",
    "    print()\n",
    "    for i in np.arange(0, num_epochs):\n",
    "        w_all.append(w)\n",
    "\n",
    "        #calculate arror due to the current weight vector:\n",
    "        err_all.append(loss(X, y, w))\n",
    "\n",
    "        #Gradient Calulation\n",
    "        dJdW = calculate_gradient(X, y, w)\n",
    "\n",
    "        # Print stats every 100 iterations\n",
    "        if (i%100) == 0:\n",
    "            print(\"iteration #: %d, loss: %4.2f\" %(i, err_all[-1]))\n",
    "\n",
    "        #Weight vector update.\n",
    "        w = update_weights(w, dJdW, lr)\n",
    "\n",
    "    return w, err_all, w_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestGradientDescent(unittest.TestCase):\n",
    "\n",
    "    def test_gradient_descend(self):\n",
    "        '''\n",
    "        Test case for weight estimation for linear regression with gradient descent method\n",
    "        '''\n",
    "\n",
    "        # Set up\n",
    "        feature_matrix = X_train\n",
    "        label_vector = y_train\n",
    "        expected_weights = np.array([4., 3.])\n",
    "\n",
    "        #call\n",
    "        w, err_all, w_all = gradient_descent(feature_matrix, label_vector, lr=0.0001, num_epochs=2000)\n",
    "\n",
    "        #asserts\n",
    "        #test the shape\n",
    "        self.assertEqual(w.shape, (2, ))\n",
    "\n",
    "        #and contents\n",
    "        np.testing.assert_array_almost_equal(expected_weights, w, decimal=0)\n",
    "\n",
    "unittest.main(argv=[''], defaultTest='TestGradientDescent', verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, err_all, w_all = gradient_descent(X_train, y_train, lr = 0.00001, num_epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_b = np.c_[np.ones((X.shape[0], 1)), X] # add X0 = 1 to each instance\n",
    "X_new = np.array([[0],[10]])\n",
    "X_new_b = np.c_[np.ones((2, 1)), X_new] # add x0 =1 to each instance\n",
    "\n",
    "for j in range(0, len(w_all)):\n",
    "    if j%10 != 0: continue\n",
    "    y_hat = predict(X_new_b, w_all[j])\n",
    "    style = \"b-\" if j > 0 else \"r--\"\n",
    "    plt.plot(X_new_b[:,1], y_hat, style)\n",
    "\n",
    "plt.plot(X_train[:, 1], y_train, \"b.\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18) \n",
    "plt.title('Gradient Descent', fontsize=18)\n",
    "plt.axis([0,10,0,40])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(err_all):\n",
    "    plt.plot(err_all)\n",
    "    plt.xlabel('Iteration #')\n",
    "    plt.ylabel('Loss: $J(\\mathbf{w})$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, err_all, w_all = gradient_descent(X_train, y_train, lr = 0.0001, num_epochs=2000 )\n",
    "plot_learning_curve(err_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variations of GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = generate_data(100)\n",
    "X_train, X_test, y_train, y_test = preprocess(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini Batch Gradient Descent(MBGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0, t1 = 200, 100000\n",
    "\n",
    "def learning_schedule(t):\n",
    "    return t0/(t + t1)\n",
    "\n",
    "def mini_batch_gd(X: np.ndarray, y: np.ndarray, num_iters: int, minibatch_size: int):\n",
    "\n",
    "    '''Estimates parametrs of linear regression model through gradient descent.\n",
    "    \n",
    "    Args:\n",
    "        1. X: Feature matrix for training data.\n",
    "        2. y: Label vector for training data.\n",
    "        3. num_iters: Number of iterations.\n",
    "\n",
    "    Returns:\n",
    "        Weight vector: Final weight vector\n",
    "        Error vector across different iterations\n",
    "        Weight vector acosss different iterations\n",
    "    '''\n",
    "\n",
    "    w_all = [] # all parameters across iterations.\n",
    "    err_all = [] # error across iterations\n",
    "\n",
    "    # Parameter vector initialised to [0,0]\n",
    "    w = np.zeros((X.shape[1]))\n",
    "    t = 0\n",
    "\n",
    "    for epoch in range(num_iters):\n",
    "        shuffled_indices = np.random.permutation(X.shape[0])\n",
    "        X_shuffled = X[shuffled_indices]\n",
    "        y_shuffled = y[shuffled_indices]\n",
    "\n",
    "        for i in range(0, X.shape[0], minibatch_size):\n",
    "            t += 1\n",
    "            xi = X_shuffled[i:i+minibatch_size]\n",
    "            yi = y_shuffled[i:i+minibatch_size]\n",
    "            err_all.append(loss(xi, yi, w))\n",
    "\n",
    "            gradients = 2/minibatch_size * calculate_gradient(xi, yi, w)\n",
    "            lr = learning_schedule(t)\n",
    "\n",
    "            w = update_weights(w, gradients, lr)\n",
    "            w_all.append(w)\n",
    "\n",
    "    return w, err_all, w_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "class TestMiniBatchGradientDescent(unittest.TestCase):\n",
    "    \n",
    "    def test_minibatch_gd(self):\n",
    "        '''\n",
    "        Test case for weight estimation for linear regression with minibatch gradient descent method.\n",
    "        '''\n",
    "\n",
    "        #set up\n",
    "        feature_matrix = X_train\n",
    "        label_vector = y_train\n",
    "        expected_weights = np.array([4., 3.])\n",
    "\n",
    "        #call\n",
    "        w, err_all, w_all = mini_batch_gd(feature_matrix, label_vector, 200, 8)\n",
    "\n",
    "        #asserts\n",
    "        #test the shape\n",
    "        self.assertEqual(w.shape, (2, ))\n",
    "\n",
    "        # And contents\n",
    "        np.testing.assert_almost_equal(expected_weights, w, decimal=0)\n",
    "    \n",
    "    unittest.main(argv=[''], defaultTest='TestMiniBatchGradientDescent', verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning curves\n",
    "\n",
    "w, err_all, w_all = mini_batch_gd(X_train, y_train, 20, 8)\n",
    "plot_learning_curve(err_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different batch sizes\n",
    "\n",
    "batch_sizes = [1, 2, 4, 8, 16, 32, 64]\n",
    "mbgd_stats = {}\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    w, err_all, w_all = mini_batch_gd(X_train, y_train, 100, batch_size)\n",
    "    mbgd_stats[batch_size] = err_all\n",
    "    legend = 'batch size: %d' %batch_size\n",
    "    plt.plot(err_all, label=legend)\n",
    "\n",
    "plt.xlabel('iteration #')\n",
    "plt.ylabel('Loss: $J(\\mathbf{w})$')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent(MBGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(X: np.ndarray, y: np.ndarray, num_epochs: int):\n",
    "\n",
    "    '''Estimates parametrs of linear regression model through stochastic gradient descent.\n",
    "    \n",
    "    Args:\n",
    "        1. X: Feature matrix for training data.\n",
    "        2. y: Label vector for training data.\n",
    "        3. num_epochs: Number of epochs.\n",
    "\n",
    "    Returns:\n",
    "        Weight vector: Final weight vector\n",
    "        Error vector across different iterations\n",
    "        Weight vector acosss different iterations\n",
    "    '''\n",
    "\n",
    "    w_all = [] # all parameters across iterations.\n",
    "    err_all = [] # error across iterations\n",
    "\n",
    "    # Parameter vector initialised to [0,0]\n",
    "    w = np.zeros((X.shape[1]))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i in range(X.shape[0]):\n",
    "            random_index = np.random.randint(X.shape[0])\n",
    "            xi = X[random_index: random_index + 1]\n",
    "            yi = y[random_index: random_index + 1]\n",
    "            err_all.append(loss(xi, yi, w))\n",
    "\n",
    "            gradients = 2 * calculate_gradient(xi, yi, w)\n",
    "            lr = learning_schedule(epoch * X.shape[0] + i)\n",
    "\n",
    "            w = update_weights(w, gradients, lr)\n",
    "            w_all.append(w)\n",
    "\n",
    "    return w, err_all, w_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestStochasticGradientDescent(unittest.TestCase):\n",
    "    \n",
    "    def test_sgd(self):\n",
    "        '''\n",
    "        Test case for weight estimation for linear regression with stochastic gradient descent method.\n",
    "        '''\n",
    "\n",
    "        #set up\n",
    "        feature_matrix = X_train\n",
    "        label_vector = y_train\n",
    "        expected_weights = np.array([4., 3.])\n",
    "\n",
    "        #call\n",
    "        w, err_all, w_all = sgd(feature_matrix, label_vector, 200, 8)\n",
    "\n",
    "        #asserts\n",
    "        #test the shape\n",
    "        self.assertEqual(w.shape, (2, ))\n",
    "\n",
    "        # And contents\n",
    "        np.testing.assert_almost_equal(expected_weights, w, decimal=0)\n",
    "    \n",
    "    unittest.main(argv=[''], defaultTest='TestMiniBatchGradientDescent', verbosity=2, exit=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, err_all, w_all = sgd(X_train, y_train, 10)\n",
    "plot_learning_curve(err_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End to end implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinReg(object):\n",
    "    '''\n",
    "    Linear regression Model\n",
    "    ------------------------------------\n",
    "    y = X@w\n",
    "    X: A feature matrix\n",
    "    w: weight vector\n",
    "    y; Label Vector\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.t0 = 200\n",
    "        self.t1 = 100000\n",
    "\n",
    "    def predict(self, X:np.ndarray) -> np.ndarray:\n",
    "        ''' Prediction of ouotput label for a given input\n",
    "\n",
    "            Args:\n",
    "                X: Feature matrix of shape (n, m+1).\n",
    "                w: weight vector of shape (m+1,)\n",
    "\n",
    "            Returns:\n",
    "                y: Predicted label vector of shape (n,) \n",
    "\n",
    "        '''\n",
    "\n",
    "        y = X @ self.w\n",
    "        return y\n",
    "\n",
    "    def loss(self, X: np.ndarray, y: np.ndarray) -> float:\n",
    "        '''Calculates loss for a model based on known labels\n",
    "    \n",
    "        Args:\n",
    "            X: Feature matrix for given inputs\n",
    "            y: Output label vector as predicted by the given model\n",
    "            w: Weight vector\n",
    "\n",
    "        Returns:\n",
    "            Loss\n",
    "        '''\n",
    "\n",
    "        err = self.predict(X) - y\n",
    "        return (1/2)*(np.transpose(err) @ err)\n",
    "\n",
    "    def rmse(self, X: np.ndarray, y: np.ndarray) -> float:\n",
    "        '''Calculate root mean square error of prediction wrt actual label.\n",
    "        \n",
    "        Args:\n",
    "            X: Feature matrix for the given inputs\n",
    "            y: output label vector as predicted by the given model\n",
    "\n",
    "        Returns:\n",
    "            Loss\n",
    "        '''\n",
    "\n",
    "        return np.sqrt((2/X.shape[0]) * self.loss(X, y))\n",
    "\n",
    "    def fit(self, X:np.ndarray, y:np.ndarray) -> float:\n",
    "        '''Estimates parameters of the linear regression model with normal equation.\n",
    "    \n",
    "        Args: \n",
    "            X: Feature matrix for given inputs.\n",
    "            y: Actual label vector.\n",
    "\n",
    "        Returns:\n",
    "            Weight vector\n",
    "        '''\n",
    "        self.w = np.linalg.pinv(X) @ y \n",
    "        return self.w\n",
    "\n",
    "    def calculate_gradient(self, X:np.ndarray, y:np.ndarray) -> np.ndarray:\n",
    "        '''Calculates gradients of loss dunction wrt weight vector on training set\n",
    "    \n",
    "        Arguments:\n",
    "\n",
    "            X: Feature matrix for training data.\n",
    "            y: Label vector for training data\n",
    "            w: Weight vector\n",
    "\n",
    "        Returns:\n",
    "            A vector of gradients.\n",
    "        '''\n",
    "\n",
    "        return np.transpose(X) @ (self.predict(X) - y)\n",
    "    \n",
    "    def update_weights(self, grad: np.ndarray, lr: float) -> np.ndarray:\n",
    "        '''\n",
    "        Updates the weights based on the gradient of loss function.\n",
    "\n",
    "        Weight updates are carried out with the fiollowing formula:\n",
    "            w_new := w_old - lr * grad\n",
    "\n",
    "        Args:\n",
    "            1. w: Weight vector\n",
    "            2. grad: gradient of loss w.r.t w\n",
    "            3. lr: learning rate\n",
    "\n",
    "        Returns:\n",
    "            Updated weight vector\n",
    "        '''\n",
    "\n",
    "        return (self.w - lr*grad)\n",
    "\n",
    "    def learning_schedule(self, t) -> float:\n",
    "        lr = (self.t0) / (t + self.t1)\n",
    "        return lr\n",
    "\n",
    "    def gd(self, X:np.ndarray, y:np.ndarray, num_epochs:int, lr:float) -> np.ndarray:\n",
    "        '''Estimates parameters of linear regression model through gradient descent.\n",
    "    \n",
    "        Arguments:\n",
    "            X: Feature matrix for training data\n",
    "            y: Label vector for training data\n",
    "            lr: learning rate\n",
    "            num_epochs: Number of training steps\n",
    "\n",
    "        Returns:\n",
    "            Weight vector: Final weight vector\n",
    "            Error vector across different iterations\n",
    "            Weight vectors acrss different iterations\n",
    "        '''\n",
    "        self.w = np.zeros((X.shape[1])) # parameter vector initialised to zero\n",
    "        self.w_all = [] #all paramenter across iterations\n",
    "        self.err_all = [] # all arrors across iterations\n",
    "\n",
    "\n",
    "\n",
    "        #Gradient descent loop\n",
    "        for i in np.arange(0, num_epochs):\n",
    "\n",
    "            #Gradient Calulation\n",
    "            dJdW = self.calculate_gradient(X, y)\n",
    "\n",
    "            self.w_all.append(self.w)\n",
    "\n",
    "            #calculate arror due to the current weight vector:\n",
    "            self.err_all.append(self.loss(X, y))\n",
    "\n",
    "            #Weight vector update.\n",
    "            self.w = self.update_weights(dJdW, lr)\n",
    "\n",
    "        return self.w\n",
    "    \n",
    "    def mbgd(self, X: np.ndarray, y: np.ndarray, num_epochs: int, batch_size: int) -> np.ndarray:\n",
    "\n",
    "        '''Estimates parametrs of linear regression model through gradient descent.\n",
    "    \n",
    "        Args:\n",
    "            1. X: Feature matrix for training data.\n",
    "            2. y: Label vector for training data.\n",
    "            3. num_iters: Number of iterations.\n",
    "\n",
    "        Returns:\n",
    "            Weight vector: Final weight vector\n",
    "            Error vector across different iterations\n",
    "         Weight vector acosss different iterations\n",
    "        \n",
    "        '''\n",
    "        # Parameter vector initialised to [0,0]\n",
    "        self.w = np.zeros((X.shape[1]))\n",
    "        \n",
    "        self.w_all = [] # all parameters across iterations.\n",
    "        self.err_all = [] # error across iterations\n",
    "\n",
    "        mini_batch_id = 0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            shuffled_indices = np.random.permutation(X.shape[0])\n",
    "            X_shuffled = X[shuffled_indices]\n",
    "            y_shuffled = y[shuffled_indices]\n",
    "\n",
    "            for i in range(0, X.shape[0], batch_size):\n",
    "                mini_batch_id += 1\n",
    "                xi = X_shuffled[i: i + batch_size]\n",
    "                yi = y_shuffled[i: i + batch_size]\n",
    "            \n",
    "                self.w_all.append(self.w)\n",
    "                self.err_all.append(self.loss(xi, yi))\n",
    "\n",
    "                dJdW = 2/batch_size * self.calculate_gradient(xi, yi)\n",
    "\n",
    "                self.w = self.update_weights(dJdW, self.learning_schedule(mini_batch_id))\n",
    "            \n",
    "        return self.w\n",
    "    \n",
    "    def sgd(self, X: np.ndarray, y: np.ndarray, num_epochs: int) -> np.ndarray:\n",
    "\n",
    "        '''Estimates parametrs of linear regression model through stochastic gradient descent.\n",
    "    \n",
    "        Args:\n",
    "            1. X: Feature matrix for training data.\n",
    "            2. y: Label vector for training data.\n",
    "            3. num_epochs: Number of epochs.\n",
    "\n",
    "        Returns:\n",
    "            Weight vector: Final weight vector\n",
    "            Error vector across different iterations\n",
    "            Weight vector acosss different iterations\n",
    "        '''\n",
    "\n",
    "        self.w_all = [] # all parameters across iterations.\n",
    "        self.err_all = [] # error across iterations\n",
    "\n",
    "        # Parameter vector initialised to [0,0]\n",
    "        self.w = np.zeros((X.shape[1]))\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            for i in range(X.shape[0]):\n",
    "                random_index = np.random.randint(X.shape[0])\n",
    "                xi = X[random_index: random_index + 1]\n",
    "                yi = y[random_index: random_index + 1]\n",
    "            \n",
    "                self. err_all.append(self.loss(xi, yi))\n",
    "                self.w_all.append(self.w)\n",
    "\n",
    "                gradients = 2 * self.calculate_gradient(xi, yi)\n",
    "                lr = self.learning_schedule(epoch * X.shape[0] + i)\n",
    "\n",
    "                self.w = self.update_weights(gradients, lr)\n",
    "            \n",
    "\n",
    "        return self.w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application on synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = generate_data(100)\n",
    "X_train, X_test, y_train, y_test = preprocess(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinReg()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "print(\"weight vector(normal equation): \", lin_reg.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg.gd(X_train, y_train, 1000, lr=1e-4)\n",
    "print(\"weight vector(gd): \", lin_reg.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg.mbgd(X_train, y_train, 1000, 16)\n",
    "print(\"weight vector(mbgd): \", lin_reg.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg.sgd(X_train, y_train, 1000)\n",
    "print(\"weight vector(sgd): \", lin_reg.w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression on multiple features and single label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X, y, coef = make_regression(n_samples=200, n_features=10, n_informative=10, n_targets=1, shuffle=True, coef=True, noise=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"shape of feature matrix; \", X.shape)\n",
    "print(\"Shape of label vector: \", y.shape)\n",
    "print(\"Shape of coef vector: \", coef.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Weight vectors used to generate data: \", coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = preprocess(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg.fit(X_train, y_train) #Normal equation for weight vector estimation\n",
    "print(\"Weight vector(normal eqn): \", lin_reg.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if the coefficient are close enough\n",
    "np.testing.assert_almost_equal(coef, lin_reg.w[1:], decimal=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg.gd(X_train, y_train, 1000, lr=1e-4)\n",
    "print(\"weight vector(gd): \", lin_reg.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(lin_reg.err_all[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg.mbgd(X_train, y_train, 1000, 16)\n",
    "print(\"weight vector(mbgd): \", lin_reg.w)\n",
    "plot_learning_curve(lin_reg.err_all[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if the coefficient are close enough\n",
    "np.testing.assert_almost_equal(coef, lin_reg.w[1:], decimal=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg.sgd(X_train, y_train, 1000)\n",
    "print(\"weight vector(mbgd): \", lin_reg.w)\n",
    "plot_learning_curve(lin_reg.err_all[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_almost_equal(coef, lin_reg.w[1:], decimal=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit ('3.8.13')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "110cc1dee26208153f2972f08a2ad52b6a56238dc66d48e87fb757ef2996db56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
