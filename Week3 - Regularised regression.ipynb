{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge and Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changes to be made wrt `LinReg` class implemented earlier\n",
    "1. Loss Function(`loss`): $ J(w) = \\frac{1}{2}(Xw - y)^T(Xw - y) + \\frac{\\lambda}{2}w^T w $\n",
    "2. Gradient(`calculate_gradient`): $\\frac{\\partial J(w)}{\\partial w} = X^T (Xw - y) + \\lambda w$\n",
    "3. Normal equation(`fit`): $w = (X^TX + \\lambda I)^{-1}X^Ty$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinReg(object):\n",
    "    '''\n",
    "    Linear regression Model\n",
    "    ------------------------------------\n",
    "    y = X@w\n",
    "    X: A feature matrix\n",
    "    w: weight vector\n",
    "    y; Label Vector\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.t0 = 200\n",
    "        self.t1 = 100000\n",
    "\n",
    "    def predict(self, X:np.ndarray) -> np.ndarray:\n",
    "        ''' Prediction of ouotput label for a given input\n",
    "\n",
    "            Args:\n",
    "                X: Feature matrix of shape (n, m+1).\n",
    "                w: weight vector of shape (m+1,)\n",
    "\n",
    "            Returns:\n",
    "                y: Predicted label vector of shape (n,) \n",
    "\n",
    "        '''\n",
    "\n",
    "        y = X @ self.w\n",
    "        return y\n",
    "\n",
    "    def loss(self, X: np.ndarray, y: np.ndarray, reg_rate: float) -> float:\n",
    "        '''Calculates loss for a model based on known labels\n",
    "    \n",
    "        Args:\n",
    "            X: Feature matrix for given inputs\n",
    "            y: Output label vector as predicted by the given model\n",
    "            w: Weight vector\n",
    "\n",
    "        Returns:\n",
    "            Loss\n",
    "        '''\n",
    "\n",
    "        err = self.predict(X) - y\n",
    "        #return (1/2) * (np.transpose(err) @ err)\n",
    "        return (1/2) * (np.transpose(err) @ err) + (reg_rate/2) * (np.transpose(self.w) @ self.w)\n",
    "\n",
    "    def rmse(self, X: np.ndarray, y: np.ndarray, reg_rate:float) -> float:\n",
    "        '''Calculate root mean square error of prediction wrt actual label.\n",
    "        \n",
    "        Args:\n",
    "            X: Feature matrix for the given inputs\n",
    "            y: output label vector as predicted by the given model\n",
    "\n",
    "        Returns:\n",
    "            Loss\n",
    "        '''\n",
    "\n",
    "        return np.sqrt((2/X.shape[0]) * self.loss(X, y, reg_rate))\n",
    "\n",
    "    def fit(self, X:np.ndarray, y:np.ndarray, reg_rate: float) -> float:\n",
    "        '''Estimates parameters of the linear regression model with normal equation.\n",
    "    \n",
    "        Args: \n",
    "            X: Feature matrix for given inputs.\n",
    "            y: Actual label vector.\n",
    "\n",
    "        Returns:\n",
    "            Weight vector\n",
    "        '''\n",
    "        self.w = np.zeros((X.shape[1]))\n",
    "        eye = np.eye(np.size(X,1))\n",
    "        self.w = np.linalg.solve(reg_rate * eye + X.T @ X, X.T @ y,)\n",
    "        return self.w\n",
    "\n",
    "    def calculate_gradient(self, X:np.ndarray, y:np.ndarray, reg_rate: float) -> np.ndarray:\n",
    "        '''Calculates gradients of loss dunction wrt weight vector on training set\n",
    "    \n",
    "        Arguments:\n",
    "\n",
    "            X: Feature matrix for training data.\n",
    "            y: Label vector for training data\n",
    "            w: Weight vector\n",
    "\n",
    "        Returns:\n",
    "            A vector of gradients.\n",
    "        '''\n",
    "\n",
    "        return np.transpose(X) @ (self.predict(X) - y) + reg_rate * self.w\n",
    "    \n",
    "    def update_weights(self, grad: np.ndarray, lr: float) -> np.ndarray:\n",
    "        '''\n",
    "        Updates the weights based on the gradient of loss function.\n",
    "\n",
    "        Weight updates are carried out with the fiollowing formula:\n",
    "            w_new := w_old - lr * grad\n",
    "\n",
    "        Args:\n",
    "            1. w: Weight vector\n",
    "            2. grad: gradient of loss w.r.t w\n",
    "            3. lr: learning rate\n",
    "\n",
    "        Returns:\n",
    "            Updated weight vector\n",
    "        '''\n",
    "\n",
    "        return (self.w - lr*grad)\n",
    "\n",
    "    def learning_schedule(self, t) -> float:\n",
    "        lr = (self.t0) / (t + self.t1)\n",
    "        return lr\n",
    "\n",
    "    def gd(self, X:np.ndarray, y:np.ndarray, num_epochs:int, lr:float, reg_rate: float) -> np.ndarray:\n",
    "        '''Estimates parameters of linear regression model through gradient descent.\n",
    "    \n",
    "        Arguments:\n",
    "            X: Feature matrix for training data\n",
    "            y: Label vector for training data\n",
    "            lr: learning rate\n",
    "            num_epochs: Number of training steps\n",
    "\n",
    "        Returns:\n",
    "            Weight vector: Final weight vector\n",
    "            Error vector across different iterations\n",
    "            Weight vectors acrss different iterations\n",
    "        '''\n",
    "        self.w = np.zeros((X.shape[1])) # parameter vector initialised to zero\n",
    "        self.w_all = [] #all paramenter across iterations\n",
    "        self.err_all = [] # all arrors across iterations\n",
    "\n",
    "\n",
    "\n",
    "        #Gradient descent loop\n",
    "        for i in np.arange(0, num_epochs):\n",
    "\n",
    "            #Gradient Calulation\n",
    "            dJdW = self.calculate_gradient(X, y, reg_rate)\n",
    "\n",
    "            self.w_all.append(self.w)\n",
    "\n",
    "            #calculate arror due to the current weight vector:\n",
    "            self.err_all.append(self.loss(X, y, 0))\n",
    "\n",
    "            #Weight vector update.\n",
    "            self.w = self.update_weights(dJdW, lr)\n",
    "\n",
    "        return self.w\n",
    "    \n",
    "    def mbgd(self, X: np.ndarray, y: np.ndarray, num_epochs: int, batch_size: int, reg_rate: float) -> np.ndarray:\n",
    "\n",
    "        '''Estimates parametrs of linear regression model through gradient descent.\n",
    "    \n",
    "        Args:\n",
    "            1. X: Feature matrix for training data.\n",
    "            2. y: Label vector for training data.\n",
    "            3. num_iters: Number of iterations.\n",
    "\n",
    "        Returns:\n",
    "            Weight vector: Final weight vector\n",
    "            Error vector across different iterations\n",
    "         Weight vector acosss different iterations\n",
    "        \n",
    "        '''\n",
    "        # Parameter vector initialised to [0,0]\n",
    "        self.w = np.zeros((X.shape[1]))\n",
    "        \n",
    "        self.w_all = [] # all parameters across iterations.\n",
    "        self.err_all = [] # error across iterations\n",
    "\n",
    "        mini_batch_id = 0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            shuffled_indices = np.random.permutation(X.shape[0])\n",
    "            X_shuffled = X[shuffled_indices]\n",
    "            y_shuffled = y[shuffled_indices]\n",
    "\n",
    "            for i in range(0, X.shape[0], batch_size):\n",
    "                mini_batch_id += 1\n",
    "                xi = X_shuffled[i: i + batch_size]\n",
    "                yi = y_shuffled[i: i + batch_size]\n",
    "            \n",
    "                self.w_all.append(self.w)\n",
    "                self.err_all.append(self.loss(xi, yi, 0))\n",
    "\n",
    "                dJdW = 2/batch_size * self.calculate_gradient(xi, yi, reg_rate)\n",
    "\n",
    "                self.w = self.update_weights(dJdW, self.learning_schedule(mini_batch_id))\n",
    "            \n",
    "        return self.w\n",
    "    \n",
    "    def sgd(self, X: np.ndarray, y: np.ndarray, num_epochs: int, reg_rate: float) -> np.ndarray:\n",
    "\n",
    "        '''Estimates parametrs of linear regression model through stochastic gradient descent.\n",
    "    \n",
    "        Args:\n",
    "            1. X: Feature matrix for training data.\n",
    "            2. y: Label vector for training data.\n",
    "            3. num_epochs: Number of epochs.\n",
    "\n",
    "        Returns:\n",
    "            Weight vector: Final weight vector\n",
    "            Error vector across different iterations\n",
    "            Weight vector acosss different iterations\n",
    "        '''\n",
    "\n",
    "        self.w_all = [] # all parameters across iterations.\n",
    "        self.err_all = [] # error across iterations\n",
    "\n",
    "        # Parameter vector initialised to [0,0]\n",
    "        self.w = np.zeros((X.shape[1]))\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            for i in range(X.shape[0]):\n",
    "                random_index = np.random.randint(X.shape[0])\n",
    "                xi = X[random_index: random_index + 1]\n",
    "                yi = y[random_index: random_index + 1]\n",
    "            \n",
    "                self. err_all.append(self.loss(xi, yi, 0))\n",
    "                self.w_all.append(self.w)\n",
    "\n",
    "                gradients = 2 * self.calculate_gradient(xi, yi, reg_rate)\n",
    "                lr = self.learning_schedule(epoch * X.shape[0] + i)\n",
    "\n",
    "                self.w = self.update_weights(gradients, lr)\n",
    "            \n",
    "\n",
    "        return self.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "class TestLossFunction((unittest.TestCase)):\n",
    "    \n",
    "    def test_loss_function(self):\n",
    "        '''Test case for loss function of linear regression'''\n",
    "\n",
    "        # Set up\n",
    "        feature_matrix = np.array([[1,3,2,5],[1,9,4,7]])\n",
    "        weight_vector = np.array([1,1,1,1])\n",
    "        label_vector = np.array([6,11])\n",
    "        reg_rate = 0.01\n",
    "        expected_loss = np.array([62.52])\n",
    "\n",
    "        # call\n",
    "        lin_reg = LinReg()\n",
    "        lin_reg.w = weight_vector\n",
    "        loss_value = lin_reg.loss(feature_matrix, label_vector, reg_rate)\n",
    "\n",
    "        #asserts\n",
    "        #test the shape\n",
    "        self.assertEqual(loss_value.shape, ())\n",
    "\n",
    "        #and contents\n",
    "        np.testing.assert_array_equal(expected_loss, loss_value)\n",
    "\n",
    "unittest.main(argv=[''], defaultTest = 'TestLossFunction', verbosity = 2, exit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "class TestCalculateGradient(unittest.TestCase):\n",
    "\n",
    "    def test_calculate_gradient(self):\n",
    "\n",
    "        '''\n",
    "        Test case for gradient descent\n",
    "        '''\n",
    "\n",
    "        #Set up\n",
    "        feature_matrix = np.array([[1,3,2,5],[1,9,4,7]])\n",
    "        weight_vector = np.array([1,1,1,1])\n",
    "        label_vector = np.array([6,11])\n",
    "        reg_rate = 0.01\n",
    "        expected_grad = np.array([15,105,50,95])\n",
    "\n",
    "        #Call\n",
    "        lin_reg = LinReg()\n",
    "        lin_reg.w = weight_vector\n",
    "        grad = lin_reg.calculate_gradient(feature_matrix, label_vector, reg_rate)\n",
    "\n",
    "        #asserts\n",
    "        #test the shape\n",
    "        self.assertEqual(grad.shape, (4, ))\n",
    "\n",
    "        #and contents\n",
    "        np.testing.assert_array_almost_equal(expected_grad, grad, decimal=0)\n",
    "\n",
    "unittest.main(argv=[''], defaultTest='TestCalculateGradient', verbosity=2,exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regularisation for fixing overfitting in polynomial regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import functools\n",
    "\n",
    "def get_combinations(x, degree):\n",
    "    return itertools.combinations_with_replacement(x, degree)\n",
    "\n",
    "def compute_new_features(items):\n",
    "    #reduce(lambda x, y: x * y, items, [1,2,3,4,5]) calculates ((((1*2)*3)*4)*5)\n",
    "    return functools.reduce(lambda x, y: x * y, items)\n",
    "\n",
    "def polynomial_transform(x, degree, logging=False):\n",
    "    # Converts to feature matrix.\n",
    "    if x.ndim == 1:\n",
    "        x = x[:, None]\n",
    "\n",
    "    x_t = x.transpose() #transposes the feature matrix\n",
    "    features = [np.ones(len(x))] # populates 1s as the first features\n",
    "\n",
    "    if logging:\n",
    "        print (\"Input:\", x)\n",
    "    \n",
    "    for degree in range(1, degree+1):\n",
    "        for items in get_combinations(x_t, degree):\n",
    "            features.append(compute_new_features(items))\n",
    "            if logging:\n",
    "                print (items, \":\", compute_new_features(items))\n",
    "\n",
    "    if logging:\n",
    "        print(np.asarray(features).transpose())\n",
    "\n",
    "    return np.asarray(features).transpose()\n",
    "\n",
    "def create_nonlin_training_set(func, sample_size, std):\n",
    "    x = np.linspace(0, 1, sample_size)\n",
    "    y = func(x) + np.random.normal(scale=std, size = x.shape)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "def nonlin(x):\n",
    "    return np.sin(2 * np.pi * x)\n",
    "\n",
    "def visualize_training_data(X_train, y_train):\n",
    "    points = np.linspace(0, 1, 100)\n",
    "    output = nonlin(points)\n",
    "\n",
    "    plt.scatter(X_train, y_train, facecolor=\"none\", edgecolors=\"b\", s= 50, label='training data')\n",
    "    plt.plot(points, output, c='g', label=\"$\\sin(2\\pi x)$\")\n",
    "    plt.xlabel('$x_1$')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def visualise_model_fit(X, y, lin_reg, degree, reg_rate):\n",
    "    '''plots trained model along with the data genration function'''\n",
    "\n",
    "    points = np.linspace(0, 1, 100)\n",
    "    output = nonlin(points)\n",
    "\n",
    "    if degree > 0:\n",
    "        plt.scatter(X, y, facecolor=\"none\", edgecolors=\"b\", s =50, label ='training data')\n",
    "\n",
    "    plt.plot(points, output, c=\"g\", label = \"$\\sin(2\\pi x)$\")\n",
    "\n",
    "    y_hat = lin_reg.predict(polynomial_transform(points, degree))\n",
    "    plt.plot(points, y_hat, c=\"r\", label=\"$h_\\mathbf{w}(x)$\") \n",
    "    plt.xlabel('$x_1$')\n",
    "    plt.ylabel('y')\n",
    "    plt.ylim(-1.5, 1.5)\n",
    "    plt.title(\"M={} $\\lambda = {}$\".format(degree, reg_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 10\n",
    "X, y = create_nonlin_training_set(nonlin, num_samples, 0.2)\n",
    "visualize_training_data(X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial models without regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(12, 8))\n",
    "sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n",
    "\n",
    "for i, degree in enumerate([0,1,3,5,7,9]):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    X_transform = polynomial_transform(X, degree)\n",
    "    lin_reg = LinReg()\n",
    "    lin_reg.fit(X_transform, y, reg_rate = 0)\n",
    "    visualise_model_fit(X, y, lin_reg, degree, reg_rate = 0)\n",
    "\n",
    "f.tight_layout()\n",
    "plt.legend(bbox_to_anchor=(1.05, 0.64), loc=2, borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\lambda = 0.01 $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(12, 8))\n",
    "sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n",
    "\n",
    "for i, degree in enumerate([0,1,3,5,7,9]):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    X_transform = polynomial_transform(X, degree)\n",
    "    lin_reg = LinReg()\n",
    "    lin_reg.fit(X_transform, y, reg_rate = 0)\n",
    "    visualise_model_fit(X, y, lin_reg, degree, reg_rate = 0)\n",
    "\n",
    "f.tight_layout()\n",
    "plt.legend(bbox_to_anchor=(1.05, 0.64), loc=2, borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run a polynomial regression of degree =9 with different regularisation rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(12, 8))\n",
    "sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n",
    "\n",
    "for i, reg_rate in enumerate([0, 1e-3, 1e-2, 1e-1, 1, 10]):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    X_transform = polynomial_transform(X, degree)\n",
    "    lin_reg = LinReg()\n",
    "    lin_reg.fit(X_transform, y, reg_rate = reg_rate)\n",
    "    visualise_model_fit(X, y, lin_reg, degree, reg_rate = reg_rate)\n",
    "\n",
    "f.tight_layout()\n",
    "plt.legend(bbox_to_anchor=(1.05, 0.64), loc=2, borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection\n",
    "\n",
    "* One model per $\\lambda$. Which one to select?\n",
    "* Process\n",
    "    * Fix the list of $\\lambda$ you want to experiment with.\n",
    "    * Divide the training data into training, validation and test.\n",
    "    * For each $\\lambda$:\n",
    "        * Train polynomial regression model with training data\n",
    "        * Calculate training and validation errors on the trained model\n",
    "    * Select the model with the lowest training and validation loss among all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def plot_reg_w(w_df):\n",
    "    ax = plt.gca()\n",
    "    ax.plot(w_df.T)\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xlim(ax.get_xlim()[::-1])\n",
    "    plt.xlabel('$\\lambda$')\n",
    "    plt.ylabel('$\\mathbf{w}$')\n",
    "    plt.title('Weights as a function of $\\lambda$')\n",
    "    plt.axis('tight')\n",
    "    plt.show()\n",
    "\n",
    "def plot_learning_curve(err_df):\n",
    "    ax = plt.gca()\n",
    "    ax.plot(err_df)\n",
    "    ax.set_xscale('log')\n",
    "    ax.set_xlim(ax.get_xlim()[::-1])\n",
    "    plt.xlabel('$\\lambda$')\n",
    "    plt.ylabel('$RMSE$')\n",
    "    plt.title('Training and validation loss as a function of $\\lambda$')\n",
    "    plt.axis('tight')\n",
    "    plt.legend(['training', 'validation'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_errors = {}\n",
    "val_errors = {}\n",
    "w_dict = {}\n",
    "degree = 9\n",
    "\n",
    "X_val = np.linspace(0, 1, 100)\n",
    "y_val = nonlin(X_val)\n",
    " \n",
    "X_transform = polynomial_transform(X, degree)\n",
    "X_val_transform = polynomial_transform(X_val, degree)\n",
    "\n",
    "for reg_rate in [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100]:   \n",
    "    lin_reg = LinReg()\n",
    "    lin_reg.fit(X_transform, y, reg_rate)\n",
    "\n",
    "    w_dict[reg_rate] = lin_reg.w\n",
    "    training_errors[reg_rate] = lin_reg.rmse(X_transform, y, reg_rate)\n",
    "    val_errors[reg_rate] = lin_reg.rmse(X_val_transform, y_val + np.random.normal(scale=0.25, size=len(y_val)), reg_rate)\n",
    "\n",
    "err_df = pd.DataFrame([training_errors, val_errors]).T\n",
    "w_df = pd.DataFrame(w_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(err_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reg_w(w_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso regression\n",
    "\n",
    "Lasso uses $L_1$ norm of weight vector instead of $L_2$ as used in ridge regression.\n",
    "\n",
    "Lasso doesn't have a closed form solution and it's loss function is not differentiable at all points. We need to use specialised algorithm  to learn lasso parameters. We will use sklearn Lasso implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "#Lasso\n",
    "reg_rate = 0.01\n",
    "lasso = Lasso(alpha = reg_rate)\n",
    "lasso.fit(X_transform, y)\n",
    "\n",
    "#Linear regression\n",
    "lin_reg = LinReg()\n",
    "_ = lin_reg.fit(X_transform, y, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare weight vectors of linear, ridge and lasso regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'LinReg': lin_reg.w, 'Ridge': w_df[0.01], 'Lasso': lasso.coef_})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "1. Lasso obtains a sparse weight vector. \n",
    "2. Ridge regression assigns small weights to each feaature.\n",
    "\n",
    "* Lasso is used for feature selection\n",
    "* While modelling relationship between features and labels,\n",
    "    * When all features are useful, use ridge regression\n",
    "    * When only a few features are useful, use lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize these three models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(20,5))\n",
    "ax1 = plt.subplot(1,3,1)\n",
    "degree = 9\n",
    "\n",
    "visualise_model_fit(X, y, lin_reg, degree, reg_rate=0)\n",
    "plt.title('Linear regression(M=9, $\\lambda=0$)')\n",
    "\n",
    "lin_reg.w = w_df[0.01]\n",
    "ax2 = plt.subplot(1, 3, 2)\n",
    "visualise_model_fit(X, y, lin_reg, degree, reg_rate=0.01)\n",
    "\n",
    "ax3 = plt.subplot(1,3,3)\n",
    "visualise_model_fit(X, y, lasso, degree, reg_rate=0.01)\n",
    "plt.title('Lasso (M=9, $\\lambda=0.01$)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at lasso with different regularisation rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(12, 8))\n",
    "sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n",
    "\n",
    "degree = 9\n",
    "X_transform = polynomial_transform(X, degree)\n",
    "\n",
    "for i, reg_rate in enumerate([0, 1e-3, 1e-2, 1e-1, 1, 10]):\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    lasso = Lasso(alpha=reg_rate)\n",
    "    lasso.fit(X_transform, y)\n",
    "    visualise_model_fit(X, y, lasso, degree, reg_rate)\n",
    "\n",
    "f.tight_layout()\n",
    "plt.legend(bbox_to_anchor=(1.05, 0.64), loc =2, borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "training_errors = {}\n",
    "val_errors = {}\n",
    "w_dict = {}\n",
    "degree = 9\n",
    "\n",
    "X_val = np.linspace(0, 1, 100)\n",
    "y_val = nonlin(X_val)\n",
    " \n",
    "X_transform = polynomial_transform(X, degree)\n",
    "X_val_transform = polynomial_transform(X_val, degree)\n",
    "\n",
    "for reg_rate in [1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100]:   \n",
    "    lasso = Lasso(alpha=reg_rate)\n",
    "    lasso.fit(X_transform, y, reg_rate)\n",
    "\n",
    "    w_dict[reg_rate] = lasso.coef_\n",
    "\n",
    "    training_errors[reg_rate] = mean_squared_error(y, lasso.predict(X_transform), squared=False)\n",
    "    val_errors[reg_rate] = mean_squared_error( y_val + np.random.normal(scale=0.25, size=len(y_val)), lasso.predict(X_val_transform), squared=False)\n",
    "\n",
    "err_df = pd.DataFrame([training_errors, val_errors]).T\n",
    "w_df = pd.DataFrame(w_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(err_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reg_w(w_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit ('3.8.13')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "110cc1dee26208153f2972f08a2ad52b6a56238dc66d48e87fb757ef2996db56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
