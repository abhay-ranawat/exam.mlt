{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sigmoid(X):\n",
    "    return 1/(1+np.exp(-X))\n",
    "\n",
    "import numpy as np\n",
    "def predict_label(X,w):\n",
    "    # Calculate the model's output\n",
    "    z = np.dot(X,w)\n",
    "    # Apply the sigmoid activation function to the model's output\n",
    "    sig = 1/(1+np.exp(-z))\n",
    "    # Assign labels to each of the samples in the test-set\n",
    "    labels = np.where(sig >= 0.75, 1, np.where(sig <= 0.25, -1, 0))\n",
    "    return labels\n",
    "\n",
    "import numpy as np\n",
    "def sigmoid (z:np.ndarray):\n",
    "    return 1/ (1+np.exp(-z))\n",
    "\n",
    "def gradient (X:np.ndarray, y:np.ndarray,w:np.ndarray, reg_rate):\n",
    "    return np.transpose(X)@(sigmoid(X@w)-y)+reg_rate * w\n",
    "\n",
    "import numpy as np\n",
    "def sigmoid (z:np.ndarray):\n",
    "    return 1/ (1+np.exp(-z))\n",
    "\n",
    "def calculate_gradient (X:np.ndarray, y:np.ndarray,w:np.ndarray, reg_rate):\n",
    "    return np.transpose(X)@(sigmoid(X@w)-y)+reg_rate * w\n",
    "\n",
    "def update_w(X, y, w, reg_rate, lr):\n",
    "    U = w - (lr * calculate_gradient(X, y, w, reg_rate))\n",
    "    return U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def naive_gaussian_predict(X_train, y_train, X_test):\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    clf = GaussianNB()\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    return y_pred\n",
    "\n",
    "import numpy as np\n",
    "def variance_estimate(X: np.ndarray, y: np.ndarray):\n",
    "    D = {}\n",
    "    for i in np.unique(y):\n",
    "        D[i] = np.var(X[y == i], axis=0)\n",
    "    return D\n",
    "\n",
    "import numpy as np\n",
    "def class_scores(y_test, y_pred, positive):\n",
    "    tp = np.sum((y_test == positive) & (y_pred == positive))\n",
    "    tn = np.sum((y_test != positive) & (y_pred != positive))\n",
    "    fp = np.sum((y_test != positive) & (y_pred == positive))\n",
    "    fn = np.sum((y_test == positive) & (y_pred != positive))\n",
    "\n",
    "    names = (\"Precision\", \"Recall\",\"Accuracy\", \"Misclassification Rate\",\"F1 score\")\n",
    "    values = [tp / (tp + fp), tp / (tp + fn), (tp + tn) / (tp + tn + fp + fn)]\n",
    "    values.append(1 - values[-1])\n",
    "    values.append(2 * values[0] * values[1] / (values[0] + values[1]))\n",
    "\n",
    "    scores = dict(zip(names,values))\n",
    "    return scores\n",
    "\n",
    "# Solution\n",
    "class NB(object):\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self._classes = np.unique(y)\n",
    "        n_classes = len(self._classes)\n",
    "\n",
    "        # calculate mean, var, and prior for each class\n",
    "        self._mean = np.zeros((n_classes, n_features), dtype = np.float64)\n",
    "        self._var = np.zeros((n_classes, n_features), dtype = np.float64)\n",
    "        self._priors = np.zeros(n_classes, dtype = np.float64)\n",
    "\n",
    "        for c in self._classes:\n",
    "            X_c = X[y == c]\n",
    "            self._mean[c, :] = X_c.mean(axis = 0)\n",
    "            self._var[c, :] = X_c.var(axis = 0)\n",
    "            self._priors[c] = X_c.shape[0] / float(n_samples)\n",
    "\n",
    "    def predict(self, X):\n",
    "        self._posterior = np.zeros((X.shape[0], len(self._classes)), dtype = np.float64)\n",
    "        for idx, x in enumerate(X):\n",
    "            for c in self._classes:\n",
    "                self._posterior[idx, c] = np.log(self._pdf(c, x)) + np.log(self._priors[c])\n",
    "        return np.argmax(self._posterior, axis = 1)\n",
    "\n",
    "    def _pdf(self, class_idx, X):\n",
    "        mean = self._mean[class_idx]\n",
    "        var = np.diag(self._var[class_idx])\n",
    "        z = np.power(2 * np.pi, X.shape[0]/2) * np.power(np.linalg.det(var), 1/2)\n",
    "        return (1 / z) * np.exp(-(1 / 2) * (X - mean).T @ (np.linalg.inv(var)) @ (X - mean))\n",
    "\n",
    "def naive_gmodel_eval(X_train, y_train, X_test, y_test):\n",
    "    gaussian_nb = NB()\n",
    "    gaussian_nb.fit(X_train, y_train)\n",
    "    y_pred = gaussian_nb.predict(X_test)\n",
    "\n",
    "    labels = np.unique(y_train)\n",
    "    D = {}\n",
    "    for label in labels:\n",
    "        D[label] = class_scores(y_test, y_pred, label)\n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def one_hot(y):\n",
    "  k = np.max(y)+1\n",
    "  M = np.zeros((len(y),k))\n",
    "  for i,val in enumerate(y):\n",
    "    M[i,val] =1\n",
    "  \n",
    "  M = np.delete(M,np.where(np.sum(M,axis=0)==0),axis=1)\n",
    "  return M\n",
    "    \n",
    "import numpy as np\n",
    "def manhattan(a,b):\n",
    "    return np.sum(abs(a - b))\n",
    "\n",
    "import numpy as np\n",
    "def softmax(Z):\n",
    "    exp_Z = np.exp(Z)\n",
    "    return exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\n",
    "\n",
    "import numpy as np\n",
    "def knn(class1, class2, x_new):\n",
    "    '''\n",
    "    Function to find in which cluster x_new belongs using 3-NN.\n",
    "    '''\n",
    "    dist1 = np.linalg.norm(class1 - x_new.T, axis=1)\n",
    "    dist2 = np.linalg.norm(class2 - x_new.T, axis=1)\n",
    "    dist = np.minimum(dist1, dist2)\n",
    "    index = np.argmin(dist)\n",
    "    if dist1[index] < dist2[index]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def hinge_loss(y_pred,y_test):\n",
    "    \"\"\"\n",
    "    Calculates the hinge loss for a binary classifier.\n",
    "    \"\"\"\n",
    "    loss = np.maximum(0, 1 - y_pred * y_test)\n",
    "    return np.mean(loss)\n",
    "\n",
    "import numpy as np\n",
    "def solve_eqn(A,b):\n",
    "    '''\n",
    "    Write a function 'solve_eqn' to obtain the weight vector\n",
    "    '''\n",
    "    A = np.column_stack((A, np.ones(A.shape[0])))\n",
    "    w = np.linalg.pinv(A) @ b\n",
    "    return w\n",
    "\n",
    "import numpy as np\n",
    "def fit(X_train,Y_train,X_test,Y_test,n_iters=100,lr=0.1):\n",
    "    \n",
    "    '''\n",
    "    Write a function 'fit' to compute gradient of the hinge loss function without regularization\n",
    "    '''       \n",
    "    n_samples, n_features = X.shape\n",
    "    w = np.zeros(n_features)\n",
    "    b = 0  \n",
    "\n",
    "    for _ in range(n_iters):\n",
    "        for idx, x_i in enumerate(X):\n",
    "        condition = Y[idx] * (np.dot(x_i, w) - b) >= 1  \n",
    "        if condition: \n",
    "            pass\n",
    "        else:\n",
    "            \n",
    "            w -= lr*np.dot(x_i, Y[idx])\n",
    "            b -= lr * Y[idx]\n",
    "\n",
    "    approx = np.dot(X_test, w) - b\n",
    "    y_pred=np.sign(approx)\n",
    "\n",
    "    accuracy = np.sum(y_pred == Y_test)/X.shape[0]\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "import numpy as np\n",
    "class fit_softSVM:\n",
    "    '''\n",
    "    Write a class named 'fit_softsvm' that implements soft margin SVM using GD. \n",
    "    '''\n",
    "    def __init__(self, C=15):\n",
    "        self.C = C\n",
    "        self.epochs = 100\n",
    "        self.learning_rate = 0.01\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "\n",
    "    def __decision_function(self, X):\n",
    "        return np.dot(X, self.w) + self.b\n",
    "\n",
    "    def __cost(self, margin):\n",
    "        return np.sum(np.maximum(0, margin))\n",
    "\n",
    "    def __margin(self, X, y):\n",
    "        return np.dot(X, self.w) + self.b - y\n",
    "\n",
    "    def fit(self, X, y, lr=0.01, epochs=100):\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "        self.b = 0\n",
    "        self.learning_rate = lr\n",
    "        self.epochs = epochs\n",
    "        for i in range(epochs):\n",
    "            for j in range(X.shape[0]):\n",
    "                margin = self.__margin(X[j], y[j])\n",
    "                if margin > 0:\n",
    "                    self.w = self.w - self.learning_rate * (X[j] * y[j])\n",
    "                    self.b = self.b - self.learning_rate * y[j]\n",
    "                else:\n",
    "                    self.w = self.w - self.learning_rate * (X[j] * y[j])\n",
    "                    self.b = self.b - self.learning_rate * y[j]\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(self.__decision_function(X))\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        return np.mean(self.predict(X) == y)\n",
    "\n",
    "def compute_accuracy(X_train, y_train, X_test,  y_test):\n",
    "    clf = fit_softSVM()\n",
    "    clf.fit(X_train, y_train)\n",
    "    return clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def predict_class(y):\n",
    "    '''\n",
    "    all the samples in a particualar node and retruns the predict class for the same node.\n",
    "    '''\n",
    "    return np.argmax(np.bincount(y))\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "def misclassification_error(y):\n",
    "    '''\n",
    "    all the samples in a particular node and return the misclassification error of the same node.\n",
    "    '''\n",
    "    num = np.argmax(np.bincount(y))\n",
    "    return 1 - np.mean(y == num)\n",
    "\n",
    "import numpy as np\n",
    "def gini_index(dict):\n",
    "    '''\n",
    "    Gini index of the same node.\n",
    "    '''\n",
    "    total = sum(dict.values())\n",
    "    gini = 1\n",
    "    for key in dict:\n",
    "        gini -= (dict[key]/total)**2\n",
    "    return gini\n",
    "\n",
    "import numpy as np\n",
    "def entropy(dict):\n",
    "    '''\n",
    "    Entropy of the same node. \n",
    "    '''\n",
    "    n = sum(dict.values())\n",
    "    p = 0\n",
    "    for value in dict.values():\n",
    "        p += -(value/n)*np.log2(value/n)\n",
    "    return p  \n",
    "\n",
    "import numpy as np\n",
    "def sseloss(y):\n",
    "    '''\n",
    "    returns the error associated with that node\n",
    "    '''\n",
    "    return np.sum(np.power(y - np.mean(y), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def similarity(res,lambda_):\n",
    "    '''\n",
    "    returns similarity score which can be used for XGBoost regression.\n",
    "    '''\n",
    "    return (np.sum(res) ** 2) / (len(res) + lambda_)\n",
    "\n",
    "import numpy as np\n",
    "def residual(y):\n",
    "    '''\n",
    "    calculates residuals for base model(taking mean of the target values).\n",
    "    '''\n",
    "    return y - np.mean(y)\n",
    "\n",
    "import numpy as np\n",
    "def accuracy(y_true,y_pred):\n",
    "    '''\n",
    "    Calculates the accuracy of classification\n",
    "    '''\n",
    "    return np.mean(y_true==y_pred)\n",
    "\n",
    "import numpy as np\n",
    "def bag(X,y):\n",
    "    '''\n",
    "    for creating q bootstrap from original dataset.\n",
    "    '''\n",
    "    n_samples = X.shape[0]\n",
    "    np.random.seed(42)\n",
    "    indices = np.random.choice(n_samples, size = n_samples, replace=True)\n",
    "    return X[indices], y[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def euclid(a,b):\n",
    "    '''\n",
    "    Euclidean distance between vectors a and b.\n",
    "    '''\n",
    "    return np.sqrt(np.sum((a - b) ** 2))\n",
    "\n",
    "import numpy as np\n",
    "def centroid(a,b):\n",
    "    '''\n",
    "    Input: a and b are numpy arrays of same shape.\n",
    "    Output: centroid of a and b as numpy array\n",
    "    '''\n",
    "    return (a+b)/2\n",
    "\n",
    "import random\n",
    "def silhoutte(a,b):\n",
    "    '''\n",
    "    to calculate silhoutte coefficient.\n",
    "    '''\n",
    "    s = (a*b)/max(a,b)\n",
    "    return s"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
