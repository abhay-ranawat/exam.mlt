{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benoulli NB: Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BernoulliNB(object):\n",
    "\n",
    "    def __init__(self, alpha=1.0):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        '''Estimates parameters for Bernoulli NB\n",
    "        Args:\n",
    "            X: Feature matrix of shape (n,m)\n",
    "            y: Label vector of shape (n,)\n",
    "\n",
    "        Returns:\n",
    "            w_{j y_c}, w_prior\n",
    "        '''\n",
    "        n_samples, n_features = X.shape\n",
    "        class_count = np.unique(y)\n",
    "        n_classes = len(class_count)\n",
    "\n",
    "        #Initialise the weight vector\n",
    "        self.w = np.zeros((n_classes, n_features), dtype=np.float64)\n",
    "        self.w_priors = np.zeros(n_classes, dtype=np.float64)\n",
    "\n",
    "        for c in range(n_classes):# Processing samples for each class seperately\n",
    "        \n",
    "            # Get example with label = c\n",
    "            X_c = X[y == c]\n",
    "\n",
    "            ## Estimating w_{j y_c} = P(x_j | y_c = c) \n",
    "            self.w[c, :] = (np.sum(X_c, axis=0) + self.alpha)/(X_c.shape[0] + 2.0 * self.alpha)\n",
    "\n",
    "            # Estimating prior\n",
    "            self.w_priors[c] = (X_c.shape[0] + self.alpha)/(float(n_samples) + n_classes * self.alpha)\n",
    "\n",
    "        print(\"Class conditional density: \", self.w)\n",
    "        print(\"Prior: \", self.w_priors)\n",
    "\n",
    "    def log_likelihood_prior_prod(self, X):\n",
    "        return X @(np.log(self.w).T) + (1 - X) @ (np.log((1 - self.w)).T) + np.log(self.w_priors)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        q = self.log_likelihood_prior_prod(X)\n",
    "        return np.exp(q) / np.expand_dims(np.sum(np.exp(q), axis=1), axis = 1)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.log_likelihood_prior_prod(X), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1,0], [0,1], [0,1], [1,0]])\n",
    "y = np.array([1, 0, 0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bernoulli_nb = BernoulliNB()\n",
    "bernoulli_nb.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bernoulli_nb.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bernoulli_nb.log_likelihood_prior_prod(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bernoulli_nb.predict_proba(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1,0], [0,1], [0,1], [1,0], [1, 1], [1, 1]])\n",
    "y = np.array([1, 0, 0, 1, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bernoulli_nb = BernoulliNB()\n",
    "bernoulli_nb.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bernoulli_nb.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bernoulli_nb.log_likelihood_prior_prod(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bernoulli_nb.predict_proba(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian NB: Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianNB(object):\n",
    "    def fit(self, X, y):\n",
    "        '''Parameter estimation for Gaussian NB'''\n",
    "\n",
    "        n_samples, n_features = X.shape\n",
    "        self._classes = np.unique(y)\n",
    "        n_classes = len(self._classes)\n",
    "\n",
    "        # Initialise mean, var, and prior for each class.\n",
    "        self._mean = np.zeros((n_classes, n_features), dtype=np.float64)\n",
    "        self._var = np.zeros((n_classes, n_features), dtype=np.float64)\n",
    "        self._priors = np.zeros(n_classes, dtype=np.float64)\n",
    "\n",
    "        for idx, c in enumerate(self._classes):\n",
    "\n",
    "            #Get examples with label y=c\n",
    "            X_c = X[y == c]\n",
    "\n",
    "            #Estimate mean from the training examples of class c.\n",
    "            self._mean[idx, :] = X_c.mean(axis=0)\n",
    "\n",
    "            #Estimate variance from the training examples of class c.\n",
    "            self._var[idx, :] = X_c.var(axis=0)\n",
    "\n",
    "            #Estimate priors.\n",
    "            self._priors[idx] = X_c.shape[0] / float(n_samples)\n",
    "\n",
    "        print(\"Mean: \", self._mean)\n",
    "        print(\"Variance: \", self._var)\n",
    "        print(\"Priors: \", self._priors)\n",
    "\n",
    "    def _calc_pdf(self, class_idx, X):\n",
    "        '''Calculates probability density for samples for class label class_idx'''\n",
    "\n",
    "        mean = self._mean[class_idx]\n",
    "        var = np.diag(self._var[class_idx])\n",
    "        z = np.power(2 * np.pi, X.shape[0]/2) * np.power(np.linalg.det(var), 1/2)\n",
    "        return (1/z) * np.exp(-(1/2)*(X - mean).T @ (np.linalg.inv(var)) @ (X - mean))\n",
    "\n",
    "    def _calc_prod_likelihood_prior(self, X):\n",
    "        '''Calculates product of likelihood and priors.'''\n",
    "\n",
    "        self._prod_likelihood_prior = np.zeros((X.shape[0], len(self._classes)), dtype=np.float64)\n",
    "\n",
    "        for x_idx, x in enumerate(X):\n",
    "            for idx, c in enumerate(self._classes):\n",
    "                self._prod_likelihood_prior[x_idx, c] = (np.log(self._calc_pdf(idx, x)) + np.log(self._priors[idx]))\n",
    "\n",
    "    def predict(self, X):\n",
    "        '''Predicts class labels for each example'''\n",
    "        \n",
    "        self._calc_prod_likelihood_prior(X)\n",
    "\n",
    "        return np.argmax(self._prod_likelihood_prior, axis=1)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        '''Calculates probability of each example belonging to different classes.'''\n",
    "\n",
    "        self._calc_prod_likelihood_prior(X)\n",
    "\n",
    "        return np.exp(self._prod_likelihood_prior) / np.expand_dims(np.sum(np.exp(self._prod_likelihood_prior), axis = 1), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification, make_blobs\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate data points\n",
    "X, y = make_blobs(n_samples = 100,\n",
    "                n_features=2,\n",
    "                centers=[[5,5],[10,10]],\n",
    "                cluster_std=1.5,\n",
    "                random_state=2)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_nb = GaussianNB()\n",
    "gaussian_nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='RdBu')\n",
    "\n",
    "xlim = (min(X[:, 0]) - 2, max(X[:, 0]) + 2 )\n",
    "ylim = (min(X[:, 1]) - 2, max(X[:, 1]) + 2 )\n",
    "\n",
    "xg = np.linspace(xlim[0], xlim[1], 60)\n",
    "yg = np.linspace(ylim[0], ylim[1], 40)\n",
    "xx, yy = np.meshgrid(xg, yg)\n",
    "Xgrid = np.vstack([xx.ravel(), yy.ravel()]).T\n",
    "\n",
    "for label, color in enumerate(['red', 'blue']):\n",
    "    mask = (y == label)\n",
    "    mu, std = X[mask].mean(0), X[mask].std(0)\n",
    "    P = np.exp(-0.5 * (Xgrid - mu)**2 / std**2).prod(1)\n",
    "    Pm = np.ma.masked_array(P, P < 0.03)\n",
    "    ax.pcolorfast(xg, yg, Pm.reshape(xx.shape), alpha=0.5, cmap=color.title() + 's')\n",
    "    ax.contour(xx, yy, P.reshape(xx.shape), levels=[0.01, 0.1, 0.5, 0.9], colors = color, alpha=0.2)\n",
    "\n",
    "ax.set(xlim=xlim, ylim=ylim)\n",
    "\n",
    "plt.title('Gaussian distribution of data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_nb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, gaussian_nb.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_nb.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples = 100,\n",
    "                    n_features =2,\n",
    "                    centers = [[5,5], [10,10], [20,20]],\n",
    "                    cluster_std = 1.5,\n",
    "                    random_state=3)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_nb_mc = GaussianNB()\n",
    "gaussian_nb_mc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, gaussian_nb_mc.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_nb_mc.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultinomialNB(object):\n",
    "    def fit(self, X, y, alpha = 1.0):\n",
    "        n_samples, n_features = X.shape\n",
    "        self._classes = np.unique(y)\n",
    "        n_classes = len(self._classes)\n",
    "\n",
    "        self.w = np.zeros((n_classes, n_features), dtype=np.float64)\n",
    "        self.w_prior = np.zeros(n_classes, dtype=np.float64)\n",
    "\n",
    "        for idx, c in enumerate(n_classes):\n",
    "            X_c = X[y == c]\n",
    "\n",
    "            total_count = np.sum(X_c, axis=1)\n",
    "            self.w[idx, :] = (np.sum(X_c, axis=0) + alpha) / (total_count + alpha * n_features)\n",
    "\n",
    "            self.w_prior[idx] = (X_c.shape[0] + alpha) / float(n_samples + alpha * n_classes)\n",
    "\n",
    "    def log_likelihood_prior_prod(self, X):\n",
    "        return X @ (np.log(self.w).T) + np.log(self.w_prior) \n",
    "\n",
    "    def predict(self, X):\n",
    "        q = self.log_likelihood_prior_prod(X)\n",
    "        return np.argmax(q, axis = 1)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        q = self.log_likelihood_prior_prod(X)\n",
    "        return np.exp(q) / np.expand_dims(np.sum(np.exp(q), axis = 1), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit ('3.8.13')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "110cc1dee26208153f2972f08a2ad52b6a56238dc66d48e87fb757ef2996db56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
