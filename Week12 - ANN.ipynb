{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks(ANN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions\n",
    "\n",
    "**Sigmoid** \n",
    "$$\n",
    "    g(z) = \\frac{1}{1+e^{-z}}\n",
    "$$\n",
    "\n",
    "**ReLU**\n",
    "$$\n",
    "    g(z) = \\begin{cases}\n",
    "                z, \\space z \\ge 0 \\\\\n",
    "                0, \\space z < 0\n",
    "        \\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "seed = 42\n",
    "rng = np.random.default_rng(seed==seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def grad_sigmoid(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def relu(z):\n",
    "    return np.where(z >= 0, z, 0)\n",
    "\n",
    "def grad_relu(z):\n",
    "    return np.where(z >= 0, 1, 0)\n",
    "\n",
    "# A dictionary of activation functions will be used while inititalising the network\n",
    "hidden_act = {'sigmoid': sigmoid, 'relu': relu}\n",
    "grad_hidden_act = {'sigmoid': grad_sigmoid, 'relu': grad_relu}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output layer\n",
    "\n",
    "**Identity**\n",
    "\n",
    "$$\n",
    "g(z) = z\n",
    "$$\n",
    "\n",
    "**Softmax**\n",
    "\n",
    "$$\n",
    "g(Z) =  \\left[ {\\begin{array}{cc}\n",
    "                    ... & ... & ...\\\\\n",
    "                    ... & \\frac{exp(Z_{ij})}{\\sum_{j=1}^k  exp(Z_{ij})} & ... \\\\\n",
    "                    ... & ... & ... \\\\\n",
    "                \\end{array} } \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity(z):\n",
    "    return z\n",
    "\n",
    "def softmax(z):\n",
    "    '''Row-wise softmax'''\n",
    "    #Check if z is a matrix\n",
    "    assert z.ndim == 2\n",
    "\n",
    "    #To prevent overflow, subtract max, row-wise\n",
    "    z -= z.max(axis=1, keepdims=True)\n",
    "\n",
    "    #Compute row-wise softmax\n",
    "    prob = np.exp(z) / np.exp(z).sum(axis = 1, keepdims = True)\n",
    "\n",
    "    # Check if each row is a probability distribution\n",
    "    assert np.allclose(prob.sum(axis=1), np.ones(z.shape[0]))\n",
    "    return prob\n",
    "\n",
    "output_act = {'softmax': softmax, 'identity': identity}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss\n",
    "\n",
    "**Least Square**\n",
    "$$\n",
    "    L(y, \\hat{y}) = \\frac{1}{2}.(\\hat{y} - y)^T.(\\hat{y} - y)\n",
    "$$\n",
    "\n",
    "**Categorical Cross Entropy**\n",
    "$$\n",
    "    L(Y, \\hat{Y}) = -\\bf{1}_n^T \\space (Y \\odot log\\hat{Y}) \\space \\bf{1}_k\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_square(y, y_hat):\n",
    "    return 0.5 * np.sum((y_hat - y) * (y_hat - y))\n",
    "\n",
    "def cce(Y, Y_hat):\n",
    "    return -np.sum(Y * np.log(Y_hat))\n",
    "\n",
    "losses = {'least_square': least_square, 'cce': cce}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "\n",
    "**Network Architecture**\n",
    "* number of layers - indexed by `l`\n",
    "    * $ l = 0 $: Input layer\n",
    "    * $ 1 \\le l \\le L -1 $: Hidden layers\n",
    "    * $l = L$: Output layer\n",
    "* number of neurons per layer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper function to compute total number of parameters in the network\n",
    "def count_params(layers):\n",
    "    num_params = 0\n",
    "    for l in range(1, len(layers)):\n",
    "        num_weights = layers[l-1] * layers[l]\n",
    "        num_biases = layers[l]\n",
    "        num_params += (num_weights + num_biases)\n",
    "\n",
    "    return num_params\n",
    "\n",
    "#Test\n",
    "assert count_params([64, 5, 10]) == (64 * 5 + 5) + (5 * 10 + 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameter Initilisation**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cv2 import EMD\n",
    "\n",
    "\n",
    "def init_params(layers):\n",
    "    num_params = count_params(layers) # Number of parameters in the network\n",
    "\n",
    "    W = [None for _ in range(len(layers))] # weights\n",
    "    b = [None for _ in range(len(layers))] # biases\n",
    "    gW = [None for _ in range(len(layers))] # gradient of loss wrt weights\n",
    "    gb = [None for _ in range(len(layers))] # grad loss wrt biases\n",
    "\n",
    "    # Sample from N(0,1) to initialise the parameters\n",
    "    theta = rng. standard_normal(num_params) # master params\n",
    "    gtheta = np.zeros(num_params) # master gradients\n",
    "    \n",
    "    #(start, end) specify the portion of the theta that corresponds to the\n",
    "    #parameter, W_l or b_l\n",
    "    start, end = 0, 0\n",
    "    for l in range(1, len(layers)):\n",
    "        #reshape the section (start, end) an assign it to W[l]\n",
    "        end = start + layers[l-1] * layers[l]\n",
    "        W[l] = theta[start: end].reshape(layers[l-1], layers[l])\n",
    "        gW[l] = gtheta[start: end].reshape(layers[l-1], layers[l])\n",
    "\n",
    "        #reshape the section(start, end) and assign it to b[l]\n",
    "        start, end = end, end + layers[l]\n",
    "        b[l] = theta[start: end].reshape(layers[l])\n",
    "        gb[l] = gtheta[start: end].reshape(layers[l])\n",
    "        start = end\n",
    "\n",
    "    return theta, gtheta, W, b, gW, gb\n",
    "\n",
    "## test init_params\n",
    "layers = [64, 32, 10]\n",
    "params = init_params([6, 32, 10])\n",
    "for l in range(1, len(layers)):\n",
    "    #Check if the weights are view of the master vector\n",
    "    assert params[2][l].base is params[0]\n",
    "    assert params[3][l].base is params[0]\n",
    "    assert params[4][l].base is params[1]\n",
    "    assert params[5][l].base is params[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass\n",
    "\n",
    "The forward pass algorithm is as follows. First, we initalise $A_0 = X$. The, we iteratively compute pre-activations and activations for each layer $l$ using the equations givenbelow:\n",
    "$$\n",
    "    Z_l = A_{l-1}W_l + b_l\n",
    "    A_l = g(Z_l)\n",
    "$$\n",
    "\n",
    "Finally, the netwrk's output is given by $\\hat{y} = A_l$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward pass\n",
    "\n",
    "For backward pass, we first initialise the gradients of the  pre activations at layer $L$ as $Z_L^{(g)} = \\hat{Y} - Y $. The other gradients can then be iteratively updated.\n",
    "\n",
    "$$\n",
    "    W_l^{(g)} = A_{l-1}^T Z_l^{(g)} \\\\\n",
    "$$\n",
    "$$\n",
    "    b_l^{(g)} = Z_l^{(g)^T} \\bf{1_n} \\\\\n",
    "$$\n",
    "$$\n",
    "    A_{l-1}^{(g)} = Z_l^{(g)} W_l^T \\\\\n",
    "$$\n",
    "$$\n",
    "    Z_{l-1}^{(g)} = A_{l-1}^{(g)} \\odot g'(Z_{l-1})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "\n",
    "    def __init__(self, layers, activation_choice='relu',\n",
    "                output_choice='softmax', loss_choice='cce'):\n",
    "        \n",
    "        self.layers = layers\n",
    "        # Parameters and gradients\n",
    "        self.theta, self.gtheta, self.W, self.b, self.gW, self.gb = init_params(layers)\n",
    "\n",
    "        #Activation functions\n",
    "        self.ghid = hidden_act[activation_choice]\n",
    "        self.grad_ghid = grad_hidden_act[activation_choice]\n",
    "        self.gout = output_act[output_choice]\n",
    "\n",
    "        #Loss\n",
    "        self.loss = losses[loss_choice]\n",
    "\n",
    "    def forward(self, X):\n",
    "        self.Z = [None for _ in range(len(self.layers))]\n",
    "        self.A = [None for _ in range(len(self.layers))]\n",
    "        self.A[0] = X\n",
    "        self.Z[0] = X\n",
    "\n",
    "        for l in range(1, len(self.layers)):\n",
    "            self.Z[l] = self.A[l-1] @ self.W[l] + self.b[l]\n",
    "            self.A[l] = self.ghid(self.Z[l])\n",
    "        \n",
    "        self.A[-1] = self.gout(self.Z[-1])\n",
    "        return self.A[-1]\n",
    "        \n",
    "    def backward(self, Y, Y_hat):\n",
    "        gZ = [None for _ in range(len(self.layers))]\n",
    "        gA = [None for _ in range(len(self.layers))]\n",
    "        gZ[-1] = Y_hat -Y\n",
    "\n",
    "        for l in range(len(self.layers) - 1, 0, -1):\n",
    "            self.gW[l][:,:] = self.A[l-1].T @ gZ[l]\n",
    "            self.gb[l][:] = np.sum(gZ[l].T, axis = 1)\n",
    "\n",
    "            gA[l-1] = gZ[l] @ self.W[l].T\n",
    "            gZ[l-1] = gA[l-1] * self.grad_ghid(self.Z[l-1])\n",
    "\n",
    "    def fit(self, X, Y,\n",
    "                lr = 0.01, epochs = 100, batch_size = 100):\n",
    "\n",
    "        self.losses = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            #Compute the loss\n",
    "            Y_hat = self.forward(X)\n",
    "            self.losses.append(self.loss(Y, Y_hat))\n",
    "            #Shuffle the dataset\n",
    "            indices = np.arange(X.shape[0])\n",
    "            rng.shuffle(indices)\n",
    "            X, Y = X[indices], Y[indices]\n",
    "            #number of batches\n",
    "            num_batches = X.shape[0] // batch_size\n",
    "            #Mini-batch GD\n",
    "            for b in range(num_batches):\n",
    "                Xb = X[b * batch_size: (b + 1) * batch_size]\n",
    "                Yb = Y[b * batch_size: (b + 1) * batch_size]\n",
    "                #Compute the prediction for this batch\n",
    "                Y_hat_b =  self.forward(Xb)\n",
    "                #Compute the gradients for this batch\n",
    "                self.backward(Yb, Y_hat_b)\n",
    "                #Update the gradients of all parameters\n",
    "                self.theta -= lr*self.gtheta\n",
    "\n",
    "    def predict(self, X):\n",
    "        Y_hat = self.forward(X)\n",
    "        if X.shape[-1] == 1: # regression\n",
    "            return Y_hat\n",
    "\n",
    "        else: #classification\n",
    "            return np.argmax(Y_hat, axis = 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstration\n",
    "\n",
    "We are using digits dataset from sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unicodedata import digit\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "\n",
    "X = digits.images\n",
    "#Normalising the data\n",
    "X /= np.max(X)\n",
    "y = digits.target\n",
    "plt.imshow(X[0])\n",
    "print(f'Sample image with label {y[0]}')\n",
    "print(X.shape)\n",
    "\n",
    "#Reshape input\n",
    "X = X.reshape(-1, 64)\n",
    "#Input size\n",
    "isize = X.shape[-1]\n",
    "#output size\n",
    "osize = len(np.unique(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_encoder(y):\n",
    "    k = len(np.unique(y))\n",
    "    return np.eye(k)[y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=seed)\n",
    "Y_train = onehot_encoder(y_train)\n",
    "Y_test = onehot_encoder(y_test)\n",
    "\n",
    "print('Training data, shape:', X_train.shape, Y_train.shape)\n",
    "print('Test data, shape:', X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [isize, 32, osize]\n",
    "network = Network(layers, activation_choice='sigmoid',\n",
    "                output_choice='softmax', loss_choice='cce')\n",
    "\n",
    "epochs = 50\n",
    "network.fit(X_train, Y_train, lr=0.01, epochs=epochs, batch_size=10)\n",
    "\n",
    "#plot losses\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.plot(range(epochs), network.losses)\n",
    "plt.title('Loss vs Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('CCE Loss')\n",
    "\n",
    "#compute accuracy\n",
    "accuracy = np.sum(network.predict(X_test) == y_test )/ X_test.shape[0]\n",
    "print(f'Test-data size = {X_test.shape[0]}')\n",
    "print(f'Accuracy = {accuracy: .2f}')\n",
    "print(f'Number of parameters = {count_params(layers)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit ('3.8.13')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "110cc1dee26208153f2972f08a2ad52b6a56238dc66d48e87fb757ef2996db56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
