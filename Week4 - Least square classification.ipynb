{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least Square Classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Polynomial Transformations\n",
    "\n",
    "import itertools\n",
    "import functools\n",
    "\n",
    "def get_combinations(x, degree):\n",
    "    return itertools.combinations_with_replacement(x, degree)\n",
    "\n",
    "def compute_new_features(items):\n",
    "    #reduce(lambda x, y: x * y, items, [1,2,3,4,5]) calculates ((((1*2)*3)*4)*5)\n",
    "    return functools.reduce(lambda x, y: x * y, items)\n",
    "\n",
    "def polynomial_transform(x, degree, logging=False):\n",
    "    # Converts to feature matrix.\n",
    "    if x.ndim == 1:\n",
    "        x = x[:, None]\n",
    "\n",
    "    x_t = x.transpose() #transposes the feature matrix\n",
    "    features = [np.ones(len(x))] # populates 1s as the first features\n",
    "\n",
    "    if logging:\n",
    "        print (\"Input:\", x)\n",
    "    \n",
    "    for degree in range(1, degree+1):\n",
    "        for items in get_combinations(x_t, degree):\n",
    "            features.append(compute_new_features(items))\n",
    "            if logging:\n",
    "                print (items, \":\", compute_new_features(items))\n",
    "\n",
    "    if logging:\n",
    "        print(np.asarray(features).transpose())\n",
    "\n",
    "    return np.asarray(features).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1,2,3])\n",
    "polynomial_transform(x, degree=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoding\n",
    "\n",
    "Since the output y is a discrete quantity, we use one-hot encoding.\n",
    "\n",
    "Let's implement `LabelTransformer` for this purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelTransformer(object):\n",
    "    \"\"\"\n",
    "    Label encoder decoder\n",
    "    -----------------------\n",
    "\n",
    "    Attributes:\n",
    "        n_classes: int\n",
    "                number of classes\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_classes:int = None):\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "    @property\n",
    "    def n_classes(self):\n",
    "        return self.__n_classes\n",
    "\n",
    "    @n_classes.setter\n",
    "    def n_classes(self, K):\n",
    "        self.__n_classes = K\n",
    "        self.__encoder = None if K is None else np.eye(K)\n",
    "\n",
    "    @property\n",
    "    def encoder(self):\n",
    "        return self.__encoder\n",
    "\n",
    "    def encode(self, class_indices: np.ndarray):\n",
    "        \"\"\"\n",
    "        encode class index into one-of-k code\n",
    "\n",
    "        Parameters\n",
    "        ----------------\n",
    "        class_indices: (N,) np.ndarray\n",
    "            non_negative class index\n",
    "            elements must be integer in [0, n_classes)\n",
    "\n",
    "        Returns\n",
    "        ----------------\n",
    "        (N, K) np.ndarray\n",
    "                one-of-k encoding of input\n",
    "        \"\"\"\n",
    "\n",
    "        if self.n_classes is None:\n",
    "            self.n_classes = np.max(class_indices) + 1\n",
    "\n",
    "        return self.encoder[class_indices]\n",
    "\n",
    "    def decode(self, onehot: np.ndarray):\n",
    "        \"\"\"\n",
    "        decode one-of-k code into class index\n",
    "\n",
    "        Parameters\n",
    "        -----------------\n",
    "        onehot : (N, K) np.ndarray\n",
    "            one-of-k code\n",
    "\n",
    "        Returns\n",
    "        ------------------\n",
    "        (N,) np.ndarray\n",
    "            class index\n",
    "        \"\"\" \n",
    "\n",
    "        return np.argmax(onehot, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstration\n",
    "\n",
    "Binary classification setup:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_labels = LabelTransformer(2).encode(np.array([1,0,1,0]))\n",
    "binary_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiclass setup with three classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiclass_labels = LabelTransformer(3).encode(np.array([1,0,1,2]))\n",
    "multiclass_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least square classification implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeastSquareClassification(object):\n",
    "    '''\n",
    "    LSC Model\n",
    "    ------------------------------------\n",
    "    y = np.where(X@w >= 0, 1, 0)\n",
    "    X: A feature matrix\n",
    "    w: weight vector\n",
    "    y; Label Vector\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.t0 = 200\n",
    "        self.t1 = 1000\n",
    "\n",
    "    def predict(self, X:np.ndarray) -> np.ndarray:\n",
    "        ''' Prediction of output label for a given input\n",
    "\n",
    "            Args:  \n",
    "                X: Feature matrix for given inputs\n",
    "\n",
    "            Returns:\n",
    "                y: Predicted label vector \n",
    "\n",
    "        '''\n",
    "        # Check to make sure that the shapes are compatible\n",
    "        assert X.shape[-1] == self.w.shape[0], f\"X shape {X.shape} and w shape {self.w.shape}, are not compatible\"\n",
    "        return np.argmax(X @ self.w, axis=-1)\n",
    "\n",
    "    def predict_internal(self, X:np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "        Prediction of output label for a given input\n",
    "\n",
    "        Args:\n",
    "            X: Feature matrix for given inputs\n",
    "\n",
    "        Returns:\n",
    "            y: Output label vector encoded in label encoder format\n",
    "        '''\n",
    "\n",
    "        #Check to make sure that the shapes are compatible\n",
    "        assert X.shape[-1] == self.w.shape[0], f\"X shape {X.shape} and w shape {self.w.shape}, are not compatible\"\n",
    "        return X @ self.w\n",
    "\n",
    "    def loss(self, X: np.ndarray, y: np.ndarray, reg_rate: float) -> float:\n",
    "        '''Calculates loss for a model based on known labels\n",
    "    \n",
    "        Args:\n",
    "            X: Feature matrix for given inputs\n",
    "            y: Output label vector as predicted by the given model\n",
    "            w: Weight vector\n",
    "\n",
    "        Returns:\n",
    "            Loss\n",
    "        '''\n",
    "\n",
    "        err = self.predict_internal(X) - y\n",
    "        #return (1/2) * (np.transpose(err) @ err)\n",
    "        return (1/2) * (np.transpose(err) @ err) + (reg_rate/2) * (np.transpose(self.w) @ self.w)\n",
    "\n",
    "    def rmse(self, X: np.ndarray, y: np.ndarray, reg_rate:float) -> float:\n",
    "        '''Calculate root mean square error of prediction wrt actual label.\n",
    "        \n",
    "        Args:\n",
    "            X: Feature matrix for the given inputs\n",
    "            y: output label vector as predicted by the given model\n",
    "\n",
    "        Returns:\n",
    "            Loss\n",
    "        '''\n",
    "\n",
    "        return np.sqrt((2/X.shape[0]) * self.loss(X, y, reg_rate))\n",
    "\n",
    "    def fit(self, X:np.ndarray, y:np.ndarray, reg_rate: float) -> float:\n",
    "        '''Estimates parameters of the linear regression model with normal equation.\n",
    "    \n",
    "        Args: \n",
    "            X: Feature matrix for given inputs.\n",
    "            y: Actual label vector.\n",
    "\n",
    "        Returns:\n",
    "            Weight vector\n",
    "        '''\n",
    "        self.w = np.zeros((X.shape[1]))\n",
    "        eye = np.eye(np.size(X,1))\n",
    "        self.w = np.linalg.solve(reg_rate * eye + X.T @ X, X.T @ y,)\n",
    "        return self.w\n",
    "\n",
    "    def calculate_gradient(self, X:np.ndarray, y:np.ndarray, reg_rate: float) -> np.ndarray:\n",
    "        '''Calculates gradients of loss dunction wrt weight vector on training set\n",
    "    \n",
    "        Arguments:\n",
    "\n",
    "            X: Feature matrix for training data.\n",
    "            y: Label vector for training data\n",
    "            w: Weight vector\n",
    "\n",
    "        Returns:\n",
    "            A vector of gradients.\n",
    "        '''\n",
    "    \n",
    "        return (np.transpose(X) @ (self.predict_internal(X) - y)) + reg_rate * self.w\n",
    "    \n",
    "    def update_weights(self, grad: np.ndarray, lr: float) -> np.ndarray:\n",
    "        '''\n",
    "        Updates the weights based on the gradient of loss function.\n",
    "\n",
    "        Weight updates are carried out with the fiollowing formula:\n",
    "            w_new := w_old - lr * grad\n",
    "\n",
    "        Args:\n",
    "            1. w: Weight vector\n",
    "            2. grad: gradient of loss w.r.t w\n",
    "            3. lr: learning rate\n",
    "\n",
    "        Returns:\n",
    "            Updated weight vector\n",
    "        '''\n",
    "\n",
    "        return (self.w - lr*grad)\n",
    "\n",
    "    def learning_schedule(self, t) -> float:\n",
    "        lr = (self.t0) / (t + self.t1)\n",
    "        return lr\n",
    "\n",
    "    def gd(self, X:np.ndarray, y:np.ndarray, num_epochs:int, lr:float, reg_rate: float) -> np.ndarray:\n",
    "        '''Estimates parameters of linear regression model through gradient descent.\n",
    "    \n",
    "        Arguments:\n",
    "            X: Feature matrix for training data\n",
    "            y: Label vector for training data\n",
    "            lr: learning rate\n",
    "            num_epochs: Number of training steps\n",
    "\n",
    "        Returns:\n",
    "            Weight vector: Final weight vector\n",
    "            Error vector across different iterations\n",
    "            Weight vectors acrss different iterations\n",
    "        '''\n",
    "        self.w = np.zeros((X.shape[1], y.shape[1])) # parameter vector initialised to zero\n",
    "        self.w_all = [] #all paramenter across iterations\n",
    "        self.err_all = [] # all arrors across iterations\n",
    "\n",
    "\n",
    "\n",
    "        #Gradient descent loop\n",
    "        for i in np.arange(0, num_epochs):\n",
    "\n",
    "            #Gradient Calulation\n",
    "            dJdW = self.calculate_gradient(X, y, reg_rate)\n",
    "\n",
    "            self.w_all.append(self.w)\n",
    "\n",
    "            #calculate arror due to the current weight vector:\n",
    "            self.err_all.append(self.loss(X, y, 0))\n",
    "\n",
    "            #Weight vector update.\n",
    "            self.w = self.update_weights(dJdW, lr)\n",
    "\n",
    "        return self.w\n",
    "    \n",
    "    def mbgd(self, X: np.ndarray, y: np.ndarray, num_epochs: int, batch_size: int, reg_rate: float) -> np.ndarray:\n",
    "\n",
    "        '''Estimates parametrs of linear regression model through gradient descent.\n",
    "    \n",
    "        Args:\n",
    "            1. X: Feature matrix for training data.\n",
    "            2. y: Label vector for training data.\n",
    "            3. num_iters: Number of iterations.\n",
    "\n",
    "        Returns:\n",
    "            Weight vector: Final weight vector\n",
    "            Error vector across different iterations\n",
    "         Weight vector acosss different iterations\n",
    "        \n",
    "        '''\n",
    "        # Parameter vector initialised to [0,0]\n",
    "        self.w = np.zeros((X.shape[1], y.shape[1]))\n",
    "        \n",
    "        self.w_all = [] # all parameters across iterations.\n",
    "        self.err_all = [] # error across iterations\n",
    "\n",
    "        mini_batch_id = 0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            shuffled_indices = np.random.permutation(X.shape[0])\n",
    "            X_shuffled = X[shuffled_indices]\n",
    "            y_shuffled = y[shuffled_indices]\n",
    "\n",
    "            for i in range(0, X.shape[0], batch_size):\n",
    "                mini_batch_id += 1\n",
    "                xi = X_shuffled[i: i + batch_size]\n",
    "                yi = y_shuffled[i: i + batch_size]\n",
    "            \n",
    "                self.w_all.append(self.w)\n",
    "                self.err_all.append(self.loss(xi, yi, 0))\n",
    "\n",
    "                dJdW = 2/batch_size * self.calculate_gradient(xi, yi, reg_rate)\n",
    "\n",
    "                self.w = self.update_weights(dJdW, self.learning_schedule(mini_batch_id))\n",
    "            \n",
    "        return self.w\n",
    "    \n",
    "    def sgd(self, X: np.ndarray, y: np.ndarray, num_epochs: int, reg_rate: float) -> np.ndarray:\n",
    "\n",
    "        '''Estimates parametrs of linear regression model through stochastic gradient descent.\n",
    "    \n",
    "        Args:\n",
    "            1. X: Feature matrix for training data.\n",
    "            2. y: Label vector for training data.\n",
    "            3. num_epochs: Number of epochs.\n",
    "\n",
    "        Returns:\n",
    "            Weight vector: Final weight vector\n",
    "            Error vector across different iterations\n",
    "            Weight vector acosss different iterations\n",
    "        '''\n",
    "\n",
    "        self.w_all = [] # all parameters across iterations.\n",
    "        self.err_all = [] # error across iterations\n",
    "\n",
    "        # Parameter vector initialised to [0,0]\n",
    "        self.w = np.zeros((X.shape[1], y.shape[1]))\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            for i in range(X.shape[0]):\n",
    "                random_index = np.random.randint(X.shape[0])\n",
    "                xi = X[random_index: random_index + 1]\n",
    "                yi = y[random_index: random_index + 1]\n",
    "            \n",
    "                self. err_all.append(self.loss(xi, yi, 0))\n",
    "                self.w_all.append(self.w)\n",
    "\n",
    "                gradients = 2 * self.calculate_gradient(xi, yi, reg_rate)\n",
    "                lr = self.learning_schedule(epoch * X.shape[0] + i)\n",
    "\n",
    "                self.w = self.update_weights(gradients, lr)\n",
    "            \n",
    "\n",
    "        return self.w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstration\n",
    "\n",
    "We will demonstrate working on leasst square classification in following set ups:\n",
    "\n",
    "1. Linearly seperable binary classification setup.\n",
    "2. Linearly seperable binary classification seup with a few outlier points.\n",
    "3. Multi-class classification with $k=3$.\n",
    "4. Polynomial least square classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a dataset for binary classification with #samples, n=50.\n",
    "\n",
    "* It also has facility to add outllier to the generated dataset.\n",
    "* and it can generate samples from multiple classes(>2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_toy_data(add_outliers = False, add_class=False):\n",
    "    x0 = np.random.normal(size=50).reshape(-1, 2) - 1\n",
    "    x1 = np.random.normal(size=50).reshape(-1, 2) + 1\n",
    "\n",
    "    if add_outliers:\n",
    "        x_1 = np.random.normal(size=10).reshape(-1, 2) + np.array([5., 10.])\n",
    "        return np.concatenate([x0, x1, x_1]), np.concatenate([np.zeros(25), np.ones(30)]).astype(int)\n",
    "    if add_class:\n",
    "        x2 = np.random.normal(size=50).reshape(-1, 2) + 2.\n",
    "        return np.concatenate([x0, x1, x2]), np.concatenate([np.zeros(25), np.ones(25), 2 + np.zeros(25)]).astype(int)\n",
    "    \n",
    "    return np.concatenate([x0, x1]), np.concatenate([np.zeros(25), np.ones(25)]).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's define a generic dataa preprocessing function that\n",
    "\n",
    "1. generates synthtic data by calling `create_toy_data` function.\n",
    "2. Perform polynomial transformation(default degree=1) on feature set.\n",
    "3. Divides the data into training and evaluation sets with `train_test_split`\n",
    "4. Performs label tranformation for both train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def preprocess(add_class=False, add_outliers=False, degree=1):\n",
    "    x, y = create_toy_data(add_outliers, add_class)\n",
    "    x_poly = polynomial_transform(x, degree=degree)\n",
    "    x_train, x_test, y_train, y_test =  train_test_split(x_poly, y)\n",
    "    y_train_trans = LabelTransformer().encode(y_train)\n",
    "    y_test_trans = LabelTransformer().encode(y_test)\n",
    "    return x_train, x_test, y_train, y_test, y_train_trans, y_test_trans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model visualisation\n",
    "\n",
    "sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n",
    "\n",
    "def visualize_model(X_train, labels, lsc_obj, degree=1):\n",
    "    \n",
    "    f = plt.figure(figsize=(8, 8))\n",
    "\n",
    "    #compute xlim and ylim\n",
    "    x1_min = np.min(x_train[:, 1])\n",
    "    x1_max = np.max(x_train[:, 1])\n",
    "    x2_min = np.min(x_train[:, 2])\n",
    "    x2_max = np.max(x_train[:, 2])\n",
    "\n",
    "    x1_test, x2_test = np.meshgrid(np.linspace(x1_min, x1_max, 100), np.linspace(x2_min, x2_max, 100))\n",
    "    x_test = np.array([x1_test, x2_test]).reshape(2, -1).T\n",
    "    x_test_poly = polynomial_transform(x_test, degree=degree)\n",
    "    y_test = lsc_obj.predict(x_test_poly)\n",
    "\n",
    "    sns.scatterplot(data=X_train, x=X_train[:,1], y=X_train[:,2], hue=labels)\n",
    "    plt.contourf(x1_test, x2_test, y_test.reshape(100, 100), alpha = 0.5,\n",
    "                levels = np.linspace(0,1,3))\n",
    "    plt.xlabel(\"$x_1$\") \n",
    "    plt.ylabel(\"$x_2$\")\n",
    "    plt.xlim(x1_min, x1_max)\n",
    "    plt.ylim(x2_min, x2_max)\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo 1: Linearly seperable binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test, y_train_trans, y_test_trans = preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nShape of training feature matrix:\", x_train.shape)\n",
    "print(\"Shape of label vector:\", y_train.shape)\n",
    "\n",
    "print(\"\\nShape of test feature matrix:\", x_test.shape)\n",
    "print(\"Shape of label vector:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"white\")\n",
    "f = plt.figure(figsize=(8, 8))\n",
    "sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n",
    "\n",
    "sns.scatterplot(data=x_train, x=x_train[:,-2], y=x_train[:,-1], hue=y_train)\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsc = LeastSquareClassification()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normal Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsc.fit(x_train, y_train_trans, reg_rate=0)\n",
    "print(\"weight_vector: \", lsc.w)\n",
    "visualize_model(x_train, y_train, lsc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(err_all):\n",
    "    err = [err[1][1] for err in err_all]\n",
    "    plt.plot(np.arange(len(err)), err, 'r-')\n",
    "    plt.xlabel('Iter #')\n",
    "    plt.ylabel('$\\mathbf{J(w)}$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsc = LeastSquareClassification()\n",
    "lsc.sgd(x_train, y_train_trans, num_epochs=100, reg_rate=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(lsc.err_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise the decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_model(x_train, y_train, lsc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsc = LeastSquareClassification()\n",
    "lsc.gd(x_train, y_train_trans, num_epochs=100, reg_rate=0, lr = 1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(lsc.err_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_model(x_train, y_train, lsc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_test.shape)\n",
    "y_test_hat = lsc.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counts of true positives and negatives, false positives and negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = np.where((y_test == 1) & (y_test_hat == 1), 1, 0).sum()\n",
    "tn = np.where((y_test == 0) & (y_test_hat == 0), 1, 0).sum()\n",
    "fp = np.where((y_test == 0) & (y_test_hat == 1), 1, 0).sum()\n",
    "fn = np.where((y_test == 1) & (y_test_hat == 0), 1, 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(tp, fp):\n",
    "    if (tp + fp) == 0: return NaN\n",
    "    return tp / (tp + fp)\n",
    "\n",
    "def recall(tp, fn):\n",
    "    if (tp + fn) == 0: return NaN\n",
    "    return tp / (tp + fn)\n",
    "\n",
    "def accuracy(tp, fp, tn, fn):\n",
    "    return (tp + tn) / (tp + fp +tn + fn) \n",
    "\n",
    "def f1_score(pr, r):\n",
    "    return 2 * ((pr * r) / (pr + r))\n",
    "\n",
    "print(\"Precision: \", precision(tp, fp))\n",
    "print(\"Recall: \", recall(tp, fn))\n",
    "print(\"accuracy: \", accuracy(tp, fp, tn, fn))\n",
    "print(\"f1 score: \", f1_score(precision(tp, fp), recall(tp, fn)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo 2: Linearly seperable binary classification with outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test, y_train_trans, y_test_trans = preprocess(add_outliers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nShape of training feature matrix:\", x_train.shape)\n",
    "print(\"Shape of label vector:\", y_train.shape)\n",
    "\n",
    "print(\"\\nShape of test feature matrix:\", x_test.shape)\n",
    "print(\"Shape of label vector:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"white\")\n",
    "f = plt.figure(figsize=(8, 8))\n",
    "sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n",
    "\n",
    "sns.scatterplot(data=x_train, x=x_train[:,-2], y=x_train[:,-1], hue=y_train)\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsc = LeastSquareClassification()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normal Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsc.fit(x_train, y_train_trans, reg_rate=0)\n",
    "print(\"weight_vector: \", lsc.w)\n",
    "visualize_model(x_train, y_train, lsc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(err_all):\n",
    "    err = [err[1][1] for err in err_all]\n",
    "    plt.plot(np.arange(len(err)), err, 'r-')\n",
    "    plt.xlabel('Iter #')\n",
    "    plt.ylabel('$\\mathbf{J(w)}$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsc = LeastSquareClassification()\n",
    "lsc.sgd(x_train, y_train_trans, num_epochs=1, reg_rate=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(lsc.err_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise the decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_model(x_train, y_train, lsc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsc = LeastSquareClassification()\n",
    "lsc.gd(x_train, y_train_trans, num_epochs=100, reg_rate=0, lr = 1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(lsc.err_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_model(x_train, y_train, lsc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_test.shape)\n",
    "y_test_hat = lsc.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counts of true positives and negatives, false positives and negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = np.where((y_test == 1) & (y_test_hat == 1), 1, 0).sum()\n",
    "tn = np.where((y_test == 0) & (y_test_hat == 0), 1, 0).sum()\n",
    "fp = np.where((y_test == 0) & (y_test_hat == 1), 1, 0).sum()\n",
    "fn = np.where((y_test == 1) & (y_test_hat == 0), 1, 0).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(tp, fp):\n",
    "    if (tp + fp) == 0: return NaN\n",
    "    return tp / (tp + fp)\n",
    "\n",
    "def recall(tp, fn):\n",
    "    if (tp + fn) == 0: return NaN\n",
    "    return tp / (tp + fn)\n",
    "\n",
    "def accuracy(tp, fp, tn, fn):\n",
    "    return (tp + tn) / (tp + fp +tn + fn) \n",
    "\n",
    "def f1_score(pr, r):\n",
    "    return 2 * ((pr * r) / (pr + r))\n",
    "\n",
    "print(\"Precision: \", precision(tp, fp))\n",
    "print(\"Recall: \", recall(tp, fn))\n",
    "print(\"accuracy: \", accuracy(tp, fp, tn, fn))\n",
    "print(\"f1 score: \", f1_score(precision(tp, fp), recall(tp, fn)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo 3: Multiclass classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test, y_train_trans, y_test_trans = preprocess(add_class=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nShape of training feature matrix:\", x_train.shape)\n",
    "print(\"Shape of label vector:\", y_train.shape)\n",
    "\n",
    "print(\"\\nShape of test feature matrix:\", x_test.shape)\n",
    "print(\"Shape of label vector:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"white\")\n",
    "f = plt.figure(figsize=(8, 8))\n",
    "sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n",
    "\n",
    "sns.scatterplot(data=x_train, x=x_train[:,-2], y=x_train[:,-1], hue=y_train)\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsc = LeastSquareClassification()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normal Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsc.fit(x_train, y_train_trans, reg_rate=0)\n",
    "print(\"weight_vector: \", lsc.w)\n",
    "visualize_model(x_train, y_train, lsc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_test_pred = lsc.predict(x_test)\n",
    "print(classification_report(y_test, y_test_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo 4: Polynomial least square classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "x, y = make_circles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_poly = polynomial_transform(x, degree=2)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_poly, y)\n",
    "\n",
    "print(\"Shape of the feature matrix before transformation: \", x.shape)\n",
    "print(\"Shape of the feature matrix after polynomial transformation: \", x_poly.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_trans = LabelTransformer().encode(y_train)\n",
    "y_test_trans = LabelTransformer().encode(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"white\")\n",
    "f = plt.figure(figsize=(8, 8))\n",
    "sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n",
    "\n",
    "sns.scatterplot(data=x_train, x=x_train[:,-2], y=x_train[:,-1], hue=y_train)\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsc_poly = LeastSquareClassification()\n",
    "lsc_poly.fit(x_train, y_train_trans, reg_rate=1)\n",
    "visualize_model(x_train, y_train, lsc_poly, degree=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_test_pred = lsc_poly.predict(x_test)\n",
    "print(classification_report(y_test, y_test_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 64-bit ('3.8.13')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "110cc1dee26208153f2972f08a2ad52b6a56238dc66d48e87fb757ef2996db56"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
