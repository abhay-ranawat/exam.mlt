{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regresion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#For reproducibility\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear combination and sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_combination(X:np.ndarray, w:np.ndarray) -> np.ndarray:\n",
    "    '''Calculates linear combination of features.\n",
    "    \n",
    "    The linear combination is calculated witht he following vectorised form\n",
    "\n",
    "    z = Xw\n",
    "\n",
    "    Args:\n",
    "        X: feature matrix with shape(n, m)\n",
    "        w: weight vector with shape(m,)\n",
    "\n",
    "    Returns:\n",
    "        Linear combination of features with shape (n,)\n",
    "    '''\n",
    "\n",
    "    return X @ w\n",
    "\n",
    "def sigmoid(z: np.ndarray) -> np.ndarray:\n",
    "    '''Calculates sigmoid of linear combination of features z\n",
    "    \n",
    "    Args:\n",
    "        z: list of floats\n",
    "\n",
    "    Returns:\n",
    "        Sigmoid function of linear combination of features as an array\n",
    "    '''\n",
    "\n",
    "    return 1/(1 + np.exp(-z))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X: np.ndarray, w:np.ndarray, threshold:float) -> np.ndarray:\n",
    "    '''Predicts class label for samples\n",
    "    \n",
    "    Args:\n",
    "        X: feature matrix with shape(n, m)\n",
    "        w: weight vector with shape(m,)\n",
    "        threshold: Probability the=reshold for prediction\n",
    "    \n",
    "    Returns:\n",
    "        Predicted class labels\n",
    "    '''\n",
    "    return np.where(sigmoid(linear_combination(X, w)) > threshold, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(X:np.ndarray, y:np.ndarray, w:np.ndarray, reg_rate:float) -> float:\n",
    "    '''Calculate loss function for a given weight vactor\n",
    "\n",
    "    Args:\n",
    "        X: feature matrix with shape(n, m)\n",
    "        y: label vector with shape(n,)\n",
    "        w: weight vector with shape(m,)\n",
    "        reg_rate: L2 regularisation rate\n",
    "\n",
    "    Returns:\n",
    "        Loss function\n",
    "    '''\n",
    "    return ((-1*np.sum(y @ np.log(linear_combination(X, w)) + (1 - y) @ np.log(1 - linear_combination(X, w)))) \n",
    "        + reg_rate * np.dot(w.T, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient of loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradient(X:np.ndarray, y:np.ndarray, w:np.ndarray, reg_rate: float) -> np.ndarray:\n",
    "    '''Calculates gradients of loss function wrt weight vector on training set\n",
    "\n",
    "    Args: \n",
    "        X: Feature matrix for training data.\n",
    "        y:Label vector for training data.\n",
    "        reg_rate: regularisation rate\n",
    "\n",
    "    returns:\n",
    "        A vector of gradients\n",
    "    '''\n",
    "\n",
    "    return X.T @ (sigmoid(linear_combination(X, w)) - y) + reg_rate*w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(w:np.ndarray, grad:np.ndarray, lr:float) -> np.ndarray:\n",
    "    '''Updates the weights based on the gradient of loss function\n",
    "    Args:\n",
    "        1. w: Weight vector\n",
    "        2. grad: gradient of loss w.r.t w\n",
    "        3.  lr: learning rate\n",
    "    Returns:\n",
    "        Updated weights\n",
    "    '''\n",
    "    return (w - lr*grad)\n",
    "\n",
    "def gd(X:np.ndarray, y:np.ndarray, num_epochs:int, lr:float, reg_rate:float) -> np.ndarray:\n",
    "    '''Estimates the parameters of logistic regression model with gradient descent'\n",
    "    \n",
    "    Args:\n",
    "        X: Feaature matrix for training data.\n",
    "        y: Label vector for traaining data.\n",
    "        num_epochs: NUmber of training steps\n",
    "        lr: Learning rate\n",
    "        reg_rate: Regularisation rate\n",
    "\n",
    "    Returns:\n",
    "        Weight vector: Final weight vector\n",
    "    '''\n",
    "\n",
    "    w = np.zeros(X.shape[1])\n",
    "    w_all = []\n",
    "    err_all = []\n",
    "\n",
    "    for i in np.arange(0, num_epochs):\n",
    "        dJdW = calculate_gradient(X, y, w, reg_rate)\n",
    "        w_all.append(w)\n",
    "        err_all.append(err_all)\n",
    "        w = update_weights(w, dJdW, lr)\n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(object):\n",
    "\n",
    "    '''Logistic Regression model\n",
    "\n",
    "        y = sigmoid(X @ w)\n",
    "    '''\n",
    "    \n",
    "    def set_weight_vector(self, w):\n",
    "        self.w = w\n",
    "    \n",
    "    def linear_combination(self, X:np.ndarray) -> np.ndarray:\n",
    "        '''Calculates linear combination of features.\n",
    "    \n",
    "        The linear combination is calculated witht he following vectorised form\n",
    "\n",
    "        z = Xw\n",
    "        Args:\n",
    "            X: feature matrix with shape(n, m)\n",
    "            w: weight vector with shape(m,)\n",
    "\n",
    "        Returns:\n",
    "            Linear combination of features with shape (n,)\n",
    "        \n",
    "        '''\n",
    "        return X @ self.w\n",
    "\n",
    "    def sigmoid(self, z: np.ndarray) -> np.ndarray:\n",
    "        '''Calculates sigmoid of linear combination of features z\n",
    "    \n",
    "         Args:\n",
    "            z: list of floats\n",
    "\n",
    "            Returns:\n",
    "                 Sigmoid function of linear combination of features as an array\n",
    "        '''\n",
    "\n",
    "        return 1/(1 + np.exp(-z))\n",
    "    \n",
    "    def activation(self, X:np.ndarray) -> np.ndarray:\n",
    "        '''Calculates sigmoid activation for logistic regression.\n",
    "        \n",
    "        Args:\n",
    "            X: Feature matrix with shape (n, m)\n",
    "\n",
    "        Returns:\n",
    "            activation vector with shape (n,)\n",
    "        '''\n",
    "        \n",
    "        return self.sigmoid(self.linear_combination(X))\n",
    "\n",
    "    def predict(self, X: np.ndarray, threshold=0.5) -> np.ndarray:\n",
    "        '''Predicts class label for samples\n",
    "\n",
    "        Args:\n",
    "            X: feature matrix with shape(n, m)\n",
    "            w: weight vector with shape(m,)\n",
    "            threshold: Probability the=reshold for prediction\n",
    "    \n",
    "        Returns:\n",
    "            Predicted class labels\n",
    "        '''\n",
    "        return np.where(self.activation(X) > threshold, 1, 0).astype(int)\n",
    "\n",
    "    def loss(self, X:np.ndarray, y:np.ndarray, reg_rate:float) -> float:\n",
    "        '''Calculate loss function for a given weight vactor\n",
    "\n",
    "        Args:\n",
    "            X: feature matrix with shape(n, m)\n",
    "            y: label vector with shape(n,)\n",
    "            w: weight vector with shape(m,)\n",
    "            reg_rate: L2 regularisation rate\n",
    "\n",
    "        Returns:\n",
    "            Loss function\n",
    "        '''\n",
    "        predicted_prob = self.activation(X)\n",
    "        return (-1 * (np.sum(y @ np.log(predicted_prob)) + (1 - y) @ np.log(1 - predicted_prob)))  + reg_rate * np.dot(self.w.T, self.w)\n",
    "\n",
    "    def calculate_gradient(self, X:np.ndarray, y:np.ndarray, reg_rate: float) -> np.ndarray:\n",
    "        '''Calculates gradients of loss function wrt weight vector on training set\n",
    "\n",
    "        Args: \n",
    "            X: Feature matrix for training data.\n",
    "            y:Label vector for training data.\n",
    "            reg_rate: regularisation rate\n",
    "\n",
    "        Returns:\n",
    "            A vector of gradients\n",
    "        '''\n",
    "\n",
    "        return X.T @ (self.activation(X) - y) + reg_rate * self.w\n",
    "\n",
    "    def update_weights(self, grad:np.ndarray, lr:float) -> np.ndarray:\n",
    "        '''Updates the weights based on the gradient of loss function\n",
    "        Args:\n",
    "            1. w: Weight vector\n",
    "            2. grad: gradient of loss w.r.t w\n",
    "            3.  lr: learning rate\n",
    "        Returns:\n",
    "            Updated weights\n",
    "        '''\n",
    "        return (self.w - lr*grad)\n",
    "\n",
    "    def gd(self, X:np.ndarray, y:np.ndarray, num_epochs:int, lr:float, reg_rate:float) -> np.ndarray:\n",
    "        '''Estimates the parameters of logistic regression model with gradient descent'\n",
    "    \n",
    "        Args:\n",
    "            X: Feaature matrix for training data.\n",
    "            y: Label vector for traaining data.\n",
    "            num_epochs: NUmber of training steps\n",
    "            lr: Learning rate\n",
    "            reg_rate: Regularisation rate\n",
    "\n",
    "        Returns:\n",
    "            Weight vector: Final weight vector\n",
    "        '''\n",
    "\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "        self.w_all = []\n",
    "        self.err_all = []\n",
    "\n",
    "        for i in np.arange(0, num_epochs):\n",
    "            \n",
    "            dJdW = self.calculate_gradient(X, y, reg_rate)\n",
    "            self.w_all.append(self.w)\n",
    "            self.err_all.append(self.loss(X, y, reg_rate))\n",
    "            self.w = self.update_weights(dJdW, lr)\n",
    "\n",
    "        return self.w \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Polynomial Transformations\n",
    "\n",
    "import itertools\n",
    "import functools\n",
    "\n",
    "def get_combinations(x, degree):\n",
    "    return itertools.combinations_with_replacement(x, degree)\n",
    "\n",
    "def compute_new_features(items):\n",
    "    #reduce(lambda x, y: x * y, items, [1,2,3,4,5]) calculates ((((1*2)*3)*4)*5)\n",
    "    return functools.reduce(lambda x, y: x * y, items)\n",
    "\n",
    "def polynomial_transform(x, degree, logging=False):\n",
    "    # Converts to feature matrix.\n",
    "    if x.ndim == 1:\n",
    "        x = x[:, None]\n",
    "\n",
    "    x_t = x.transpose() #transposes the feature matrix\n",
    "    features = [np.ones(len(x))] # populates 1s as the first features\n",
    "\n",
    "    if logging:\n",
    "        print (\"Input:\", x)\n",
    "    \n",
    "    for degree in range(1, degree+1):\n",
    "        for items in get_combinations(x_t, degree):\n",
    "            features.append(compute_new_features(items))\n",
    "            if logging:\n",
    "                print (items, \":\", compute_new_features(items))\n",
    "\n",
    "    if logging:\n",
    "        print(np.asarray(features).transpose())\n",
    "\n",
    "    return np.asarray(features).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo #1: Logistic regression for linearly seperable binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_toy_data():\n",
    "    x0 = np.random.normal(size=50).reshape(-1, 2) - 1\n",
    "    x1 = np.random.normal(size=50).reshape(-1, 2) + 1\n",
    "    return np.concatenate([x0, x1]), np.concatenate([np.zeros(25), np.ones(25)]).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix, label_vector = create_toy_data()\n",
    "print(\"Shape of feature matrix:\", feature_matrix.shape)\n",
    "print(\"Shape of label vector:\", label_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix_bias = polynomial_transform(feature_matrix, degree=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(feature_matrix_bias, label_vector, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert x_train.shape[0] == y_train.shape[0]\n",
    "assert x_test.shape[0] == y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualisation\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "f = plt.figure(figsize=(8,8))\n",
    "sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n",
    "\n",
    "sns.scatterplot(data=x_train, x=x_train[:,-2], y = x_train[:,-1], hue = y_train)\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression()\n",
    "log_reg.gd(x_train, y_train, num_epochs=1000, lr =1e-2, reg_rate=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(err):\n",
    "    plt.plot(np.arange(len(err)), err, 'r-')\n",
    "    plt.xlabel('Iter #')\n",
    "    plt.ylabel('$\\mathbf{J(w)}$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(log_reg.err_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize_model(X_train, labels, lsc_obj, degree=1):\n",
    "    \n",
    "    f = plt.figure(figsize=(8, 8))\n",
    "\n",
    "    #compute xlim and ylim\n",
    "    x1_min = np.min(x_train[:, 1])\n",
    "    x1_max = np.max(x_train[:, 1])\n",
    "    x2_min = np.min(x_train[:, 2])\n",
    "    x2_max = np.max(x_train[:, 2])\n",
    "\n",
    "    x1_test, x2_test = np.meshgrid(np.linspace(x1_min, x1_max, 100), np.linspace(x2_min, x2_max, 100))\n",
    "    x_test = np.array([x1_test, x2_test]).reshape(2, -1).T\n",
    "    x_test_poly = polynomial_transform(x_test, degree=degree)\n",
    "    y_test = lsc_obj.predict(x_test_poly)\n",
    "\n",
    "    sns.scatterplot(data=X_train, x=X_train[:,1], y=X_train[:,2], hue=labels)\n",
    "    plt.contourf(x1_test, x2_test, y_test.reshape(100, 100), alpha = 0.5,\n",
    "                levels = np.linspace(0,1,3))\n",
    "    plt.xlabel(\"$x_1$\") \n",
    "    plt.ylabel(\"$x_2$\")\n",
    "    plt.xlim(x1_min, x1_max)\n",
    "    plt.ylim(x2_min, x2_max)\n",
    "    plt.gca().set_aspect('equal', adjustable='box')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_model(x_train, y_train, log_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_hat = log_reg.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_test_hat, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo #2: Non linearly seperable case "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "feature_matrix, label_vector = make_circles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert feature_matrix.shape[0] == label_vector.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_poly = polynomial_transform(feature_matrix, degree=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"# features before transformation: \", feature_matrix.shape)\n",
    "print(\"# features before transformation: \", x_poly.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train,  y_test = train_test_split(x_poly, label_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert x_train.shape[0] == y_train.shape[0]\n",
    "assert x_test.shape[0] == y_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualisation\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "f = plt.figure(figsize=(8,8))\n",
    "sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n",
    "\n",
    "sns.scatterplot(data=x_train, x=x_train[:,-2], y = x_train[:,-1], hue = y_train)\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_poly = LogisticRegression()\n",
    "log_reg_poly.gd(x_train, y_train, num_epochs=10000, lr =1e-2, reg_rate=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(log_reg_poly.err_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_model(x_train, y_train, log_reg_poly, degree=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_test_hat = log_reg_poly.predict(x_test)\n",
    "print(classification_report(y_test, y_test_hat, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
